---
title: 循环神经网络
date: 2023/11/24
categories:
  - AI
tags:
  - rnn
mathjax: true
---


## 循环神经网络（RNN）

总结：
1. RNN这个R(循环)，可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。
2. 常规的RNN一般是输入和输出是等长序列，为了适应不等长序列。出现了Encoder-Decoder，也叫Seq2Seq。里面的编码器和解码器可以是rnn，也可以是rnn的变种（lstm,gru）等。
3. lstm、gru等都是RNN的变式。主要是为了解决RNN无法处理长序列的缺点。


### RNN
[rnn+lstm](https://blog.csdn.net/v_JULY_v/article/details/89894058)

[rnn](https://zhuanlan.zhihu.com/p/30844905)

[rnn+seq2seq2+attention](https://www.jiqizhixin.com/articles/2018-12-14-4)

在实际应用中，我们还会遇到很多序列形的数据：


如：

1. 自然语言处理问题。x1可以看做是第一个单词，x2可以看做是第二个单词，依次类推。
2. 语音处理。此时，x1、x2、x3……是每帧的声音信号。
3. 时间序列问题。例如每天的股票价格等等。

**而其中，序列形的数据就不太好用原始的神经网络处理了。所以为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。**

一个简单的循环神经网络如下所示：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-3884f344d71e92d70ec3c44d2795141f_720w.webp)

我们现在这样来理解，如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的全连接神经网络。x是一个向量，它表示输入层的值（这里面没有画出来表示神经元节点的圆圈）；s是一个向量，它表示隐藏层的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；

U是输入层到隐藏层的权重矩阵，o也是一个向量，它表示输出层的值；V是隐藏层到输出层的权重矩阵。

那么，现在我们来看看W是什么。循环神经网络的隐藏层的值s不仅仅取决于当前这次的输入x，还取决于上一次隐藏层的值s。权重矩阵 W就是隐藏层上一次的值作为这一次的输入的权重。

我们给出这个抽象图对应的具体图：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-206db7ba9d32a80ff56b6cc988a62440_720w.webp)

如果我们把上面的图展开，循环神经网络也可以画成下面这个样子：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-b0175ebd3419f9a11a3d0d8b00e28675_720w.webp)

现在看上去就比较清楚了，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$ 。关键一点是,$s_t$的值不仅仅取决于$x_t$ ，还取决于$s_{t-1}$ 。我们可以用下面的公式来表示循环神经网络的计算方法：

用公式表示如下：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-9524a28210c98ed130644eb3c3002087_720w.png)

### RNN的局限：长期依赖（Long-TermDependencies）问题

一些更加复杂的场景。比如我们试着去预测“I grew up in France...I speak fluent French”最后的词“French”。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的“France”的上下文。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。

不幸的是，在这个间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/38adde9a4649e5ea3809f10306d56bb0.png)

在理论上，RNN绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN则没法太好的学习到这些知识。Bengio,etal.(1994)等人对该问题进行了深入的研究，他们发现一些使训练RNN变得非常困难的相当根本的原因。

换句话说， RNN 会受到短时记忆的影响。如果一条序列足够长，那它们将很难将信息从较早的时间步传送到后面的时间步。

因此，如果你正在尝试处理一段文本进行预测，RNN 可能从一开始就会遗漏重要信息。在反向传播期间（反向传播是一个很重要的核心议题，本质是通过不断缩小误差去更新权值，从而不断去修正拟合的函数），RNN 会面临梯度消失的问题。

**因为梯度是用于更新神经网络的权重值（新的权值 = 旧权值 - 学习率*梯度），梯度会随着时间的推移不断下降减少，而当梯度值变得非常小时，就不会继续学习。**

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/7e5e9272c2f6db440c4092bca444057f.png)​

换言之，在递归神经网络中，获得小梯度更新的层会停止学习—— 那些通常是较早的层。 由于这些层不学习，**RNN会忘记它在较长序列中以前看到的内容，因此RNN只具有短时记忆。**

而梯度爆炸则是因为计算的难度越来越复杂导致。

然而，幸运的是，有个RNN的变体——LSTM，可以在一定程度上解决梯度消失和梯度爆炸这两个问题！

### LSTM
[原文链接](https://blog.csdn.net/v_JULY_v/article/details/89894058)
### LSTM变体
[原文链接](https://blog.csdn.net/v_JULY_v/article/details/89894058)
### Seq2Seq
[原文链接](https://www.jianshu.com/p/80436483b13b)
