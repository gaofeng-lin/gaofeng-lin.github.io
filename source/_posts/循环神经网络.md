---
title: 循环神经网络
date: 2023/11/24
categories:
  - AI
tags:
  - rnn
mathjax: true
abbrlink: 7224
---


## 循环神经网络（RNN）

总结：
1. RNN这个R(循环)，可以被看做是同一神经网络的多次复制，每个神经网络模块会把消息传递给下一个。
2. 常规的RNN一般是输入和输出是等长序列，为了适应不等长序列。出现了Encoder-Decoder，也叫Seq2Seq。里面的编码器和解码器可以是rnn，也可以是rnn的变种（lstm,gru）等。
3. lstm、gru等都是RNN的变式。主要是为了解决RNN无法处理长序列的缺点。


### RNN

[人人都能看懂的LSTM - 陈诚的文章 - 知乎](https://zhuanlan.zhihu.com/p/32085405)


![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-f716c816d46792b867a6815c278f11cb_r.png)

这里：

x为当前状态下数据的输入， h表示接收到的上一个节点的输入。

y为当前节点状态下的输出，而h`为传递到下一个节点的输出。



通过上图的公式可以看到，输出 h' 与 x 和 h 的值都相关。

而 y 则常常使用 h' 投入到一个线性层（主要是进行维度映射）然后使用softmax进行分类得到需要的数据。

对这里的y如何通过 h' 计算得到往往看具体模型的使用方式。



通过序列形式的输入，我们能够得到如下形式的RNN。


![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-71652d6a1eee9def631c18ea5e3c7605_r.jpg)


其他rnn图：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-206db7ba9d32a80ff56b6cc988a62440_720w.webp)

如果我们把上面的图展开，循环神经网络也可以画成下面这个样子：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-b0175ebd3419f9a11a3d0d8b00e28675_720w.webp)

现在看上去就比较清楚了，这个网络在t时刻接收到输入$x_t$之后，隐藏层的值是$s_t$，输出值是$o_t$ 。关键一点是,$s_t$的值不仅仅取决于$x_t$ ，还取决于$s_{t-1}$ 。我们可以用下面的公式来表示循环神经网络的计算方法：

用公式表示如下：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-9524a28210c98ed130644eb3c3002087_720w.png)

[rnn+lstm](https://blog.csdn.net/v_JULY_v/article/details/89894058)

[rnn](https://zhuanlan.zhihu.com/p/30844905)

[rnn+seq2seq2+attention](https://www.jiqizhixin.com/articles/2018-12-14-4)




### RNN的局限：长期依赖（Long-TermDependencies）问题

一些更加复杂的场景。比如我们试着去预测“I grew up in France...I speak fluent French”最后的词“French”。当前的信息建议下一个词可能是一种语言的名字，但是如果我们需要弄清楚是什么语言，我们是需要先前提到的离当前位置很远的“France”的上下文。这说明相关信息和当前预测位置之间的间隔就肯定变得相当的大。

不幸的是，在这个间隔不断增大时，RNN会丧失学习到连接如此远的信息的能力。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/38adde9a4649e5ea3809f10306d56bb0.png)

在理论上，RNN绝对可以处理这样的长期依赖问题。人们可以仔细挑选参数来解决这类问题中的最初级形式，但在实践中，RNN则没法太好的学习到这些知识。Bengio,etal.(1994)等人对该问题进行了深入的研究，他们发现一些使训练RNN变得非常困难的相当根本的原因。

换句话说， RNN 会受到短时记忆的影响。如果一条序列足够长，那它们将很难将信息从较早的时间步传送到后面的时间步。

因此，如果你正在尝试处理一段文本进行预测，RNN 可能从一开始就会遗漏重要信息。在反向传播期间（反向传播是一个很重要的核心议题，本质是通过不断缩小误差去更新权值，从而不断去修正拟合的函数），RNN 会面临梯度消失的问题。

**因为梯度是用于更新神经网络的权重值（新的权值 = 旧权值 - 学习率*梯度），梯度会随着时间的推移不断下降减少，而当梯度值变得非常小时，就不会继续学习。**

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/7e5e9272c2f6db440c4092bca444057f.png)​

换言之，在递归神经网络中，获得小梯度更新的层会停止学习—— 那些通常是较早的层。 由于这些层不学习，**RNN会忘记它在较长序列中以前看到的内容，因此RNN只具有短时记忆。**

而梯度爆炸则是因为计算的难度越来越复杂导致。

然而，幸运的是，有个RNN的变体——LSTM，可以在一定程度上解决梯度消失和梯度爆炸这两个问题！

### LSTM

#### 总体框架
[人人都能看懂的LSTM - 陈诚的文章 - 知乎](https://zhuanlan.zhihu.com/p/32085405)


![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-e4f9851cad426dfe4ab1c76209546827_r.png)


LSTM有两个传输状态，一个$c^t$（cell state），和一个 $h^t$（hidden state）

无论是rnn 还是 lstm ,h^t 感觉表示的都是短期记忆，rnn相当于lstm中的最后一个“输出门”的操作，是lstm的一个特例，也就是lstm中的短期记忆知识，而lstm包含了长短期的记忆，其中 C^t就是对前期记忆的不断加工，锤炼和理解，沉淀下来的，而h^t只是对前期知识点的短暂记忆，是会不断消失的。

问题二：C^t 之所以变化慢，主要是对前期记忆和当前输入的线性变换，对前期记忆的更新和变化（可以视为理解或者领悟出来的内容），是线性变换，所以变动不大，而 h^t是做的非线性变化，根据输入节点内容和非线性变化函数的不同，变动固然很大

#### 深入LSTM结构
首先使用LSTM的当前输入$x^t$和上一个状态传递下来的$h^{t-1}$拼接训练得到四个状态。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-15c5eb554f843ec492579c6d87e1497b_r.jpg)

$z^i$:输入门
$z^f$:忘记门
$z^o$:输出门

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-556c74f0e025a47fea05dc0f76ea775d_r.png)

圆圈点代表阵中对应的元素相乘
圆圈加号表示矩阵加法

LSTM内部主要有三个阶段：

1. 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。具体来说是通过计算得到的$z^f$来作为忘记门控，来控制上一个状态的$c^{t-1}$哪些需要留哪些需要忘。

2. 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入$x^{t}$进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的$z$表示。而选择的门控信号则是由 
$z^i$（i代表information）来进行控制。将上面两步得到的结果相加，即可得到传输给下一个状态的 $c^t$。也就是上图中的第一个公式。
1. 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过$z^o$ 来进行控制的。并且还对上一阶段得到的$c^o$ 进行了放缩（通过一个tanh激活函数进行变化）。

与普通RNN类似，输出$y^t$往往最终也是通过$h^t$ 变化得到。

### LSTM变体
[原文链接](https://blog.csdn.net/v_JULY_v/article/details/89894058)
### Seq2Seq
[原文链接](https://www.jianshu.com/p/80436483b13b)
