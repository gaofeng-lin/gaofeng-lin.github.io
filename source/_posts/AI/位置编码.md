---
title: 位置编码
date: 2025/11/11
categories:
  - AI
tags:
  - 循环神经网络位置编码
  - Transformer
mathjax: true
abbrlink: 6dda7187
---

# 背景
位置编码主要是解决注意力机制中的permutation invarient（置换不变）带来的问题。为了弄清楚位置编码，需要先补充一些背景知识

## permutation invarient

置换不变：随意调换输入中词的位置，并不会让注意力权重发生变化，注意力层的整个结果维持不变。具体来说，每一个词向量计算结果不变，仅仅是**词向量在输出矩阵**的排列随着词和词位置互换而对应调整了一下。词向量输出矩阵可参考下图：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-11-04_11-24-45.png)

这样会带来一个问题，给定同一组（多集）源词向量，不论它们在输入里怎么排，模型（不含位置信息）对它们的整体作用是一样的，输出结果是确定的。比如一个翻译任务，翻译“猫坐在垫子上”和翻译“垫子坐在猫上面”，不加位置编码，输出的结果是一样的。

接下来从公式角度说明这是为什么

## 注意力机制中的permutation equivariant


假设输入$X\in R^{n*d}$，权重$W_Q,W_K,W_V$，令：

$Q=XW_Q, K=XW_K, V=XW_V, S=\frac{Q K^{\top}}{\sqrt{d_{k}}}, A=\operatorname{softmax}_{\mathrm{row}}(S)$

输出为$Y=AV$

取任意置换矩阵（permutation matrix）$P\in R^{n*n}$，置换后的输入$X^{\prime}=P X$，有：

$Q^{\prime}=PQ, K^{\prime}=PK, V^{\prime}=PV$

>这里提一下置换矩阵，置换矩阵的每一行和每一列都恰好有一个1，其余元素都是0的方阵。**当一个矩阵乘上一个置换矩阵时，所得到的是原来矩阵的横行（置换矩阵在左）或纵列（置换矩阵在右）经过置换后得到的矩阵**。在注意力机制中，每行代表一个token，所以这里选择左乘置换矩阵，行置换。

注意力打分：

$S^{\prime}=\frac{Q^{\prime}K^{\prime \top}}{\sqrt{d_{k}}}=\frac{(P Q)(P K)^{\top}}{\sqrt{d_{k}}}=\frac{P Q K^{\top}P^{\top}}{\sqrt{d_{k}}}=P S P^{\top}$

因为Softmax是对每一行进行Softmax，不是整个矩阵进行。左乘$P$会行重排，右乘$P^\top$会列重排，但是不影响最后结果，所以：

$A^{\prime}=\mathrm{softmax_{tow}}(S^{\prime})=\mathrm{softmax_{tow}}(P S P^{\top})=P\mathrm{softmax_{tow}}(S)\,P^{\top}=P A P\top$

$Y^{\prime}=A^{\prime}V^{\prime}=(P A P{\top})(P V)=P(AV)=PY$

可以看到，置换输入，输出结果也会进行相应置换。那么这是不是和前面提到的permutation invarient矛盾？

其实不然，我们回顾下permutation invarient，核心是“每一个词向量计算结果不变”。也就是每个token对其他token的加权和不变，只是和$Y$相比，现在变为了$PY$。

假设之前的$Y$如下：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-11-11_11-12-58.png)

那么置换输入后得到的结果，可能变成了“爱吃我梨子”这个排列顺序的矩阵，但是每个词向量的计算结果不变。

所以，单独看注意力机制，其实是等变（equivariant）的。也就是输入做置换，输出也会被同步置换$Y^{\prime}=PY$

但是这样，输出结果还是和原始的不一样，能保证最终的输出结果一样吗？

**答案是可以的，要分具体的任务来讲述**


## permutation invarient导致的输出确定

### 分类任务

对于分类任务来说，会对注意力权重矩阵进行聚合，也就是执行sum/mean/attention pooling，因为矩阵元素一样，所以最后得到的结果也是一样的。

### 翻译任务

虽然注意力权重矩阵被置换，但解码器的 cross-attention 会把“换位”抵消掉，生成的翻译（在同样的解码策略和同样的目标前缀下）不变。
> 需要注意的是，对翻译任务来说。decoder侧的输入，是基于label的。具体来说，假设样本是：“我/爱/机器/学习”和 "i/ love /machine/ learning"。把“我/爱/机器/学习”输入到encoder里去，，而decoder则是"i/ love /machine/ learning"的右移掩码矩阵。所以下面的公式证明中，并不会对解码器的输入乘以置换矩阵。

令解码器第$t$步的查询为$Q_t$，encoder侧给出key/value:

$K=XW^{(k)}, V=XW^{(v)}$

若用置换矩阵$P$打乱源序:

$K^{\prime}=PK, V^{\prime}=PV$


cross-attention 的权重（对每一行做 softmax）：

$S_{t}=\mathrm{Softmax}_{\mathrm{row}}(Q_{t}K^{\top})$

$S_{t}^{\prime}=\mathrm{Softmax}_{\mathrm{row}}(Q_{t}K^{\prime\top})=\mathrm{Softmax}_{\mathrm{row}}(Q_{t}K^{\top}P^{\top})=S_{t}P^{\top}$


于是输出的上下文向量

${C}_{t}={S}_{t}{V}$
${C}_{t}^{\prime}={S}_{t}^{\prime}{V}^{\prime}=({S}_{t}P^{\top})(P{V})={S}_{t}{V}={C}_{t}$

cross-attention后的加权和完全一样，解码器看到的源信息对置换不敏感。自注意力是等变（输出会重排），但cross-attention 的读出是不变（置换对消）。

## 结论

给定相同词向量（猫，坐在，垫子，上），不论它们在输入里怎么排，模型（不含位置信息）对它们的整体作用是一样的，输出结果是确定的，无法准确建模，需要用位置编码来记录位置信息。


提到permutation invarient这个词，脑海当中应该想到的是“每个词向量的计算结果相同”，仅仅是在矩阵中的位置被置换了，但是最后得到的输出结果是一样的。

**那么关键的点为：没有位置编码，交换词顺序，每个词向量的计算结果仍相同。词向量的计算结果相同导致了最后的输出结果相同**。

**每个词向量的计算结果直接影响最后的输出**。

# 位置编码（Position Encoder）

## 为什么加上PE，位置信息就不会丢失

因为改变了注意力分数的计算方式。但是严格来说，PE还是会丢失部分位置信息。

模型的输入是由词嵌入和位置编码组合而成。

我们假设位置编码矩阵是$PE$，在原始输入和置换后输入两种情况下，添加位置编码：
$\tilde{X}=X+E^{\mathrm{pos}}$
$\tilde{X^{\prime}}=X^{\prime} + E^{\mathrm{pos}} = PX + E^{\mathrm{pos}}$

在进行注意力分数的计算，会发现两者的结果已经不是置换的关系。
最终结果是会包含位置信息，不再仅仅取决于词向量矩阵。


## 常见的位置编码

### 绝对位置编码

-   Sinusoidal (三角函数) 位置编码：
-   可学习的 (Learned) 绝对位置编码


### 相对位置编码

这类编码认为：“词A在位置5，词B在位置8” 这个绝对信息，不如 “词B在词A后面3个位置” 这个相对信息重要。

这类方法不再是 X_i = E_i + P_i。

直接修改（hack）注意力机制。当计算 score(i, j) 时，它们会额外注入一个专门表示 i 和 j 之间**相对距离（i - j）**的向量。

代表模型： Transformer-XL, T5, DeBERTa。



### 旋转位置编码

