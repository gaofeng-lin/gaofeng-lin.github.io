---
title: 位置编码
date: 2025/11/11
categories:
  - AI
tags:
  - 循环神经网络位置编码
  - Transformer
mathjax: true
abbrlink: 6dda7187
---

# 背景
位置编码主要是解决注意力机制中的permutation invarient（置换不变）带来的问题。为了弄清楚位置编码，需要先补充一些背景知识

## permutation invarient

置换不变：随意调换输入中词的位置，并不会让注意力权重发生变化，注意力层的整个结果维持不变。具体来说，每一个词向量计算结果不变，仅仅是**词向量在输出矩阵**的排列随着词和词位置互换而对应调整了一下。词向量输出矩阵可参考下图：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-11-04_11-24-45.png)

这样会带来一个问题，给定同一组（多集）源词向量，不论它们在输入里怎么排，模型（不含位置信息）对它们的整体作用是一样的，输出结果是确定的。比如一个翻译任务，翻译“猫坐在垫子上”和翻译“垫子坐在猫上面”，不加位置编码，输出的结果是一样的。

接下来从公式角度说明这是为什么

## 注意力机制中的permutation equivariant


假设输入$X\in R^{n*d}$，权重$W_Q,W_K,W_V$，令：

$Q=XW_Q, K=XW_K, V=XW_V, S=\frac{Q K^{\top}}{\sqrt{d_{k}}}, A=\operatorname{softmax}_{\mathrm{row}}(S)$

输出为$Y=AV$

取任意置换矩阵（permutation matrix）$P\in R^{n*n}$，置换后的输入$X^{\prime}=P X$，有：

$Q^{\prime}=PQ, K^{\prime}=PK, V^{\prime}=PV$

>这里提一下置换矩阵，置换矩阵的每一行和每一列都恰好有一个1，其余元素都是0的方阵。**当一个矩阵乘上一个置换矩阵时，所得到的是原来矩阵的横行（置换矩阵在左）或纵列（置换矩阵在右）经过置换后得到的矩阵**。在注意力机制中，每行代表一个token，所以这里选择左乘置换矩阵，行置换。

注意力打分：

$S^{\prime}=\frac{Q^{\prime}K^{\prime \top}}{\sqrt{d_{k}}}=\frac{(P Q)(P K)^{\top}}{\sqrt{d_{k}}}=\frac{P Q K^{\top}P^{\top}}{\sqrt{d_{k}}}=P S P^{\top}$

因为Softmax是对每一行进行Softmax，不是整个矩阵进行。左乘$P$会行重排，右乘$P^\top$会列重排，但是不影响最后结果，所以：

$A^{\prime}=\mathrm{softmax_{tow}}(S^{\prime})=\mathrm{softmax_{tow}}(P S P^{\top})=P\mathrm{softmax_{tow}}(S)\,P^{\top}=P A P\top$

$Y^{\prime}=A^{\prime}V^{\prime}=(P A P{\top})(P V)=P(AV)=PY$

可以看到，置换输入，输出结果也会进行相应置换。那么这是不是和前面提到的permutation invarient矛盾？

其实不然，我们回顾下permutation invarient，核心是“每一个词向量计算结果不变”。也就是每个token对其他token的加权和不变，只是和$Y$相比，现在变为了$PY$。

假设之前的$Y$如下：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-11-11_11-12-58.png)

那么置换输入后得到的结果，可能变成了“爱吃我梨子”这个排列顺序的矩阵，但是每个词向量的计算结果不变。

所以，单独看注意力机制，其实是等变（equivariant）的。也就是输入做置换，输出也会被同步置换$Y^{\prime}=PY$

但是这样，输出结果还是和原始的不一样，能保证最终的输出结果一样吗？

**答案是可以的，要分具体的任务来讲述**


## permutation invarient导致的输出确定

### 分类任务

对于分类任务来说，会对注意力权重矩阵进行聚合，也就是执行sum/mean/attention pooling，因为矩阵元素一样，所以最后得到的结果也是一样的。

### 翻译任务

虽然注意力权重矩阵被置换，但解码器的 cross-attention 会把“换位”抵消掉，生成的翻译（在同样的解码策略和同样的目标前缀下）不变。
> 需要注意的是，对翻译任务来说。decoder侧的输入，是基于label的。具体来说，假设样本是：“我/爱/机器/学习”和 "i/ love /machine/ learning"。把“我/爱/机器/学习”输入到encoder里去，，而decoder则是"i/ love /machine/ learning"的右移掩码矩阵。所以下面的公式证明中，并不会对解码器的输入乘以置换矩阵。

令解码器第$t$步的查询为$Q_t$，encoder侧给出key/value:

$K=XW^{(k)}, V=XW^{(v)}$

若用置换矩阵$P$打乱源序:

$K^{\prime}=PK, V^{\prime}=PV$


cross-attention 的权重（对每一行做 softmax）：

$S_{t}=\mathrm{Softmax}_{\mathrm{row}}(Q_{t}K^{\top})$

$S_{t}^{\prime}=\mathrm{Softmax}_{\mathrm{row}}(Q_{t}K^{\prime\top})=\mathrm{Softmax}_{\mathrm{row}}(Q_{t}K^{\top}P^{\top})=S_{t}P^{\top}$


于是输出的上下文向量

${C}_{t}={S}_{t}{V}$
${C}_{t}^{\prime}={S}_{t}^{\prime}{V}^{\prime}=({S}_{t}P^{\top})(P{V})={S}_{t}{V}={C}_{t}$

cross-attention后的加权和完全一样，解码器看到的源信息对置换不敏感。自注意力是等变（输出会重排），但cross-attention 的读出是不变（置换对消）。

## 结论

给定相同词向量（猫，坐在，垫子，上），不论它们在输入里怎么排，模型（不含位置信息）对它们的整体作用是一样的，输出结果是确定的，无法准确建模，需要用位置编码来记录位置信息。


提到permutation invarient这个词，脑海当中应该想到的是“每个词向量的计算结果相同”，仅仅是在矩阵中的位置被置换了，但是最后得到的输出结果是一样的。

**那么关键的点为：没有位置编码，交换词顺序，每个词向量的计算结果仍相同。词向量的计算结果相同导致了最后的输出结果相同**。

**每个词向量的计算结果直接影响最后的输出**。

# 位置编码（Position Encoder）

## 为什么加上PE，位置信息就不会丢失

因为改变了注意力分数的计算方式。但是严格来说，PE还是会丢失部分位置信息。

模型的输入是由词嵌入和位置编码组合而成。

我们假设位置编码矩阵是$PE$，在原始输入和置换后输入两种情况下，添加位置编码：
$\tilde{X}=X+E^{\mathrm{pos}}$
$\tilde{X^{\prime}}=X^{\prime} + E^{\mathrm{pos}} = PX + E^{\mathrm{pos}}$

在进行注意力分数的计算，会发现两者的结果已经不是置换的关系。
最终结果是会包含位置信息，不再仅仅取决于词向量矩阵。


## 位置编码设计历史

-   用整型值标记位置：给第一个token标记1，给第二个token标记2...，以此类推。导致的问题：模型可能遇见比训练时所用的序列更长的序列。不利于模型的泛化。模型的位置表示是无界的。随着序列长度的增加，位置值会越来越大。
-   用[0,1]范围标记位置：解决了整型值标记位置的问题，但是当序列长度不一样时，token间的相对距离不同。在序列长度为3时，token间的相对距离为0.5；在序列长度为4时，token间的相对距离就变为0.33。相对位置表明句子中每个token之间的距离，不应该随着句子长度变化而变化。

到这里，位置编码的设计理念应该遵从：
-   唯一性：每一个位置（时间步）都必须有一个独一无二的位置编码，不能重复。
-   可扩展性：编码方式应该能自然地推广到比训练时见过的序列更长的序列。
-   确定性：PE必须是确定的（非随机的），不随输入的改变而变化。


## 二进制编码

考虑到位置信息作用在input embedding上，因此比起用单一的值，更好的方案是用一个和input embedding维度一样的向量来表示位置。这时我们就很容易想到二进制编码。

位置 pos,"二进制表示 (d=3, d=2, d=1, d=0)"
0,0 0 0 0
1,0 0 0 1
2,0 0 1 0
3,0 0 1 1
4,0 1 0 0
5,0 1 0 1
6,0 1 1 0
7,0 1 1 1
8,1 0 0 0

transformer中的d_model足够大（一般为512），很难出现不够用的情况。每个位置的编码也是唯一和确定的。

但是存在一个问题，”这样编码出来的位置向量，处在一个离散的空间中，不同位置间的变化是不连续的。“

具体来说：

pos=6 -> [0, 1, 1, 0]

pos=7 -> [0, 1, 1, 1]

pos=8 -> [1, 0, 0, 0]

模型会看到：PE(6) 和 PE(7) 非常相似（只差1位，距离很近）。但 PE(7) 和 PE(8) 极其不相似（差了4位，距离很远）。模型会认为PE(6) -> PE(7) 是一种变换，而 PE(7) -> PE(8) 是另一种完全不同的变换，无法学习”相邻“这个概念。而在NLP中，“相邻”比“不相邻”理论是具有更高的注意力分数，所以这种“不连续性”对建模是不利的。



所以，我们希望位置编码是具有“连续性”的，

## 基于三角函数(sin)的位置编码

目前位置，位置编码需要具备：

-   唯一性：每一个位置（时间步）都必须有一个独一无二的位置编码，不能重复。
-   确定性：PE的值应该只取决于位置本身，而不是句子的长度。如果采用[0,1]
-   连续性
-   易于外推

比较容易想到的就是三角函数。不过单独的三角函数虽然具备以上性质，但是却无法满足另一个重要特点：易学习的相对位置。在NLP中，相对位置比绝对位置要更加重要。比如，“猫坐在垫子上，很可爱啊” 和 “很可爱啊，猫坐在垫子上”。词的绝对位置不同，但是语义上没有差别，不会对建模有影响。但是相对位置如果变化，可能会导致语义混乱。例如，垫子坐在猫上。而单独的sin函数无法完全表达。

以下通过公式来证明：






## 单独三角函数的缺陷

相对位置不容易学习到。

在NLP中，相对位置比绝对位置要更加重要。比如，“猫坐在垫子上，很可爱啊” 和 “很可爱啊，猫坐在垫子上”。词的绝对位置不同，但是语义上没有差别，不会对建模有影响。但是相对位置如果变化，可能会导致语义混乱。例如，垫子坐在猫上。所以单独的sin函数无法完全表达

## sin+cos的位置编码



## 位置编码的设计要求

以NLP任务为例，设计要求主要遵循：

-   唯一性：每一个位置（时间步）都必须有一个独一无二的位置编码，不能重复。
-   确定性：PE的值应该只取决于位置本身，而不是句子的长度。如果采用[0,1]
-   易于学习相对位置
-   易于外推



## 为什么单独使用sin不行

之所以要学习

我们首先要明确目标：我们希望找到一个函数 $f$，使得：$$PE(\text{pos} + k) = f(PE(\text{pos}), k)
$$并且我们要求 $f$ 必须是**线性**的，即：

$$PE(\text{pos} + k) = \mathbf{M}\_k \cdot PE(\text{pos})
$$其中，$\mathbf{M}_k$ 是一个只依赖于相对距离 $k$ 的**固定矩阵**。

如果这个条件成立，模型就可以通过学习 $\mathbf{M}_k$ 来理解相对位置 $k$，而无需管绝对位置 $\text{pos}$ 是多少。

-----

### 二、 单独 $\sin$ 的失败之处：缺少 $90^\circ$ 伴侣

现在，我们只看第 $i$ 个维度，它由 $\omega_i$ 控制频率。
我们定义：

  * $A = \text{pos} \cdot \omega_i$
  * $B = k \cdot \omega_i$

在单独 $\sin$ 的设计中，我们的位置编码值是：
$$P_{\text{sin}}(\text{pos}, i) = \sin(A)$$我们希望找到 $P_{\text{sin}}(\text{pos} + k, i)$ 与 $P_{\text{sin}}(\text{pos}, i)$ 之间的关系。根据和角公式：$$P_{\text{sin}}(\text{pos} + k, i) = \sin(A + B) = \sin(A) \cos(B) + \cos(A) \sin(B)$$将 $\sin(A)$ 替换为 $P_{\text{sin}}(\text{pos}, i)$：$$P_{\text{sin}}(\text{pos} + k, i) = P_{\text{sin}}(\text{pos}, i) \cdot \cos(B) + \cos(A) \cdot \sin(B)
$$#### 失败分析 1： $\cos(A)$ 项无法被表示（**不封闭性**）

请看上面的公式：

1.  $\cos(B)$ 和 $\sin(B)$ 只依赖于 $k$（因为 $B = k \cdot \omega_i$），所以它们是常数（对于固定的 $k$ 来说）。**这是好的。**
2.  $P_{\text{sin}}(\text{pos}, i)$ 是我们拥有的位置信息。**这也是好的。**
3.  **灾难性的项：** $\cos(A) = \cos(\text{pos} \cdot \omega_i)$。

为了计算 $P_{\text{sin}}(\text{pos} + k, i)$，模型需要知道**位置 $\text{pos}$ 处的 $\cos$ 值** $\cos(A)$。

**问题是：** 在单独 $\sin$ 的设计中，$\cos(A)$ **并没有被编码在** $PE(\text{pos})$ 向量里！

这意味着：

* 这个变换**不能**写成 $\mathbf{M}_k \cdot PE(\text{pos})$。
* 模型为了理解 $k$ 带来的变化，必须在 $PE$ 向量之外，**重新计算或推断**出 $\cos(A)$ 的值。这打破了位置编码的自包含性（Self-Contained）。

因此，单独的 $\sin$ 维度无法满足“通过 $PE(\text{pos})$ 的线性变换得到 $PE(\text{pos}+k)$”的硬性要求。

-----

### 三、 $\sin/\cos$ 配对的成功之处：二维子空间的封闭性

现在，我们来看 $\sin/\cos$ 配对（即 $\text{Sinusoidal PE}$）是如何通过增加一个维度，来解决这个问题的。

在 $\text{Sinusoidal PE}$ 中，我们把两个维度 $2i$ 和 $2i+1$ 绑定起来，形成一个**二维子空间**：

* 维度 $2i$ 存储 $\sin(A)$：$P(\text{pos}, 2i) = \sin(A)$
* 维度 $2i+1$ 存储 $\cos(A)$：$P(\text{pos}, 2i+1) = \cos(A)$

现在我们计算 $P(\text{pos}+k)$ 的两个分量：

1.  **偶数维度 $P(\text{pos}+k, 2i)$：**
$$$$P(\\text{pos}+k, 2i) = \\sin(A+B) = \\sin(A) \\cos(B) + \\cos(A) \\sin(B)

$$
$$**替换：** $\sin(A)$ 变成 $P(\text{pos}, 2i)$，$\cos(A)$ 变成 $P(\text{pos}, 2i+1)$。

$$
$$P(\\text{pos}+k, 2i) = P(\\text{pos}, 2i) \\cdot \\cos(B) + P(\\text{pos}, 2i+1) \\cdot \\sin(B)

$$
$$
奇数维度 $P(\text{pos}+k, 2i+1)$：$$ \\ P(\text{pos}+k, 2i+1) = \cos(A+B) = \cos(A) \cos(B) - \sin(A) \sin(B)
$$$$$$替换：$$ \\ P(\text{pos}+k, 2i+1) = P(\text{pos}, 2i+1) \cdot \cos(B) - P(\text{pos}, 2i) \cdot \sin(B)
$$$$$$成功分析：线性代数的胜利将这两个公式写成矩阵形式：$$\begin{bmatrix}
P(\text{pos}+k, 2i) \\
P(\text{pos}+k, 2i+1)
\end{bmatrix}
=
\begin{bmatrix}
\cos(B) & \sin(B) \\
-\sin(B) & \cos(B)
\end{bmatrix}
\begin{bmatrix}
P(\text{pos}, 2i) \\
P(\text{pos}, 2i+1)
\end{bmatrix}
$$**完美实现！**

* **封闭性：** 计算 $P(\text{pos}+k)$ 的两个分量，**只需要**用到 $P(\text{pos})$ 的这两个分量。不需要任何额外的、未编码的信息。
* **线性变换：** 两个向量之间通过一个 $2\times 2$ 的矩阵相乘来关联。
* **相对性：** 变换矩阵（旋转矩阵）中的 $\cos(B)$ 和 $\sin(B)$ 只依赖于 $B = k \cdot \omega_i$，因此**只依赖于相对距离 $k$**。

**这就是为什么必须是 $\sin$ 和 $\cos$ 交替配对。 $\cos$ 维度充当了 $\sin$ 维度的“正交伴侣”（或“相位差 $90^\circ$ 的伴侣”），共同确保了在二维子空间内，相对位置的平移（加 $k$）可以被表示为一个简单的线性代数操作（旋转）。** 模型只需要学习和应用这个旋转矩阵 $\mathbf{M}_k$，就实现了对相对位置的理解。$$

### 绝对位置编码

-   Sinusoidal (三角函数) 位置编码：Transformer中的位置编码。
-   可学习的 (Learned) 绝对位置编码：创建一个位置编码矩阵，位置编码是模型的参数，通过反向传播从数据中学习得到。优点是简单，模型自己学习最好的位置表示；缺点是无法外推，需要大量数据才能学好。


### 相对位置编码

这类编码认为：“词A在位置5，词B在位置8” 这个绝对信息，不如 “词B在词A后面3个位置” 这个相对信息重要。

这类方法不再是 X_i = E_i + P_i。

直接修改（hack）注意力机制。当计算 score(i, j) 时，它们会额外注入一个专门表示 i 和 j 之间**相对距离（i - j）**的向量。

代表模型： Transformer-XL, T5, DeBERTa。



### 旋转位置编码

