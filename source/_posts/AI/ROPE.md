---
title: ROPE
date: 2025/11/21
categories:
  - AI
tags:
  - ROPE
mathjax: true
abbrlink: 878b3a83
---

# 基础知识

## 三角函数

## 旋转矩阵 
https://zhuanlan.zhihu.com/p/183973440

XY坐标系中，向量OP旋转β角度到了OP'的位置：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-11-21_11-08-17.png)

根据三角函数关系，可以列出向量OP与OP'的坐标表示形式：

对比上面个两个式子，将第2个式子展开：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-cdf8b5fa36af46cdd4986cdbc3ec8d2a_r.png)


用矩阵形式重新表示为：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-8803684cd287390b2b689c128d76563b_1440w.jpg)

中间的矩阵即二维旋转的旋转矩阵，坐标中的某一向量左乘该矩阵后，即得到这个向量旋转β角后的坐标。

# 论文核心逻辑

-   背景：Transformer 的自注意力机制（Self-Attention）计算的是 Query 和 Key 的内积。我们需要把位置信息 $m$ 和 $n$ 塞进 $q$ 和 $k$ 里。
-   目标：我们希望找到一种编码方式 $f(x, pos)$，使得 Query 和 Key 做内积（Attention Score）之后，结果只包含相对位置 $(m-n)$ 的信息。即：$\text{InnerProduct}(q_m, k_n)$ 应该等价于一个函数 $g(\dots, m-n)$。这个函数$g(\dots, m-n)$就是约束，经过位置编码后的 $q_m$ 和 $k_n$ 的内积，应该等于一个只与“相对距离 ($m-n$)”有关的函数 $g$：
$$\langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, m-n)$$
-   推导：
    -   先在 2维空间（复数域）推导，发现 旋转（Rotation） 完美满足上述条件。
    -   再推广到 $d$ 维空间，通过将向量切分为 $d/2$ 个子空间，分别旋转。
- 性质 (Properties, Sec 3.3): 这种旋转编码自带“远程衰减”特性，符合语言规律。

# 一些问题

## 问题1

论文公式（4）中的$f_q(x_m, m) = (W_q x_m) e^{im\theta}$。看起来有些不对，因为$W_q$一般是右乘词嵌入矩阵。右乘的时候，我们默认词嵌入矩阵$x_m$是一个行向量；而论文这里使用左乘，应该是将向量 $x$ 视为列向量。

## 问题2

公式2下面的公式，咋一看有些不对，但是仔细看，可以这样去理解。

$O_m$是第 $m$ 个位置的注意力分数输出，固定$m$，然后遍历整个序列$N$，所以这个公式是没有问题的。

# 公式解读

## 公式1

$q_m = f_q(x_m, m)$

$x_m$表示的是第$m$个token，而$m$表示这个token的位置索引。

$x_m$只是一个向量，没有位置。

ROPE需要拿着 $x_m$ （向量），根据 $m$ （位置整数）计算出一个旋转角度 $m\theta$，然后把向量转一下。如果不传 $m$，函数就不知道该转多少度。



## 公式18a

论文写的是$R_q(x_q,m) R_k(x_k,m) = R_q(x_q,x_k,0)$

应该是写错了，按照上面的公式，推导下来应该是：

$R_q(x_q,m) R_k(x_k,m) = R_g(x_q,x_k,0)$


按照论文的推导，可以得到：

$$R_g(x_q, x_k, 0) =  R_q(x_q, 0) \cdot R_k(x_k, 0) =||q|| \cdot ||k|| =$$

比较难理解的在于$R_q(x_q, 0) \cdot R_k(x_k, 0) =||q|| \cdot ||k||$ 


**可以这样理解：**

公式14，论文原文解释说，可以理解为“带有空位置信息的向量” (vectors with empty position information encoded) 。也就是说，$q$ 就是还没有旋转过的初始 Query 向量。

再看公式17，是在初始条件下（m=0）的情况下得到的。

$$\underbrace{q}_{\text{向量符号}} = \underbrace{||q|| e^{i\theta_q}}_{\text{向量的极坐标表示}} = \underbrace{R_q(x_q, 0) e^{i\Theta_q(x_q, 0)}}_{\text{函数的极坐标定义}}$$

前一个都好推导，因为这里是假设二维平面，$q$是一个二维向量，用极坐标写出来就好。
-   中间项 $||q|| e^{i\theta_q}$：这是复数 $q$ 的标准写法。
    -   $||q||$ 表示向量 $q$ 的模长（也就是长度）。
    -   $\theta_q$ 表示向量 $q$ 的原本角度。

后一部分，需要和公式15结合，把m=0代入就好。

-   右边项 $R_q(x_q, 0) e^{i\Theta_q(x_q, 0)}$：这是位置编码函数 $f_q$ 在 $m=0$ 时的写法。
    -  $R_q(x_q, 0)$ 是函数定义的模长部分。
    -   $\Theta_q(x_q, 0)$ 是函数定义的角度部分。

要想让公式成立，那么模长和模长要相等，辐角和辐角也相等。所以：

$$R_q(x_q, 0) = ||q||$$

$$R_k(x_k, 0) = ||k||$$

在这个基础上，回到公式18a就能得到：

$$R_g(x_q, x_k, 0) =  R_q(x_q, 0) \cdot R_k(x_k, 0) =||q|| \cdot ||k|| =$$


那么也就推出了，在m=n的条件下：
$$R_q(x_q, m)=R_k(x_k, 0)=||q|| \cdot ||k||$$

## 公式16第二行

这个公式是公式15和公式16的结合，理论上应该是下面这个才对：

$$\Theta_k(x_k, n) + \Theta_q(x_q, m) = \Theta_g(x_q, x_k, n-m)$$

而公式16第二行给出的是：

$$\Theta_k(x_k, n) - \Theta_q(x_q, m) = \Theta_g(x_q, x_k, n-m)$$

**关键在于为什么是减号而不是加号？**

原因在于$q_m$取了转置，在复平面上，就是共轭转置

## 维度和向量

向量的解释有很多种，空间中的箭头、有序的列表等都是正确的：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-09-26_11-09-38.png)

如果在平面当中，这就是一个二维向量。对于每个token来说，它是一个$d_model$维向量。**这里一定要和矩阵区分开来**。

在ROPE的论文里面，为了方便推导，先从d=2，也就是二维向量开始推导。然后推广到多维。


## 3.4小节解读

首先3.4小节明确了是二维的情况。按照公式1的说法：
$q_m = f_q(x_m, m)$

$x_m$表示的是第$m$个token，而$m$表示这个token的位置索引。

$x_m$只是一个向量，没有位置。

那么$x_q$是一个二维向量，而且论文把q进行转置，说明是按照列排序，而不是传统的行排序。行排序是w进行转置。

所以在3.4小节中，$x_q$是一个竖排向量，转置后就是横排，和$k_n$做内积，得到的是一个标量。

因为作者借助复数来推导，所以后面的内积是复数域的内积。

对于二维向量来说，内积就是对应部分乘积然后相加。

内积的符号：<>

a=[3,2]
b=[4,6]
<a,b> = 12+12=24

推广到复数域：a=3+2i; b=4+6i

如果直接计算内积：<a,b>=12+26i-12
因为内积是标量，所以会对计算结果取实部，但是结果显然不对，应该乘以后面那个复数的共轭。

<a,b> = Re[ab*]

------------------------

公式12：$x_q$是一个二维向量，列排列，表示的是某个token，$q_m$就是query，$k_n$是key。$f_q$和$f_k$就是论文的目标：找到一种编码方式 $f(x, pos)$，使得 Query 和 Key 做内积（Attention Score）之后，结果只包含相对位置 $(m-n)$ 的信息。**所以看到$f_q$和$f_k$，可以进一步理解为，根据输入$x_q$和其位置索引m，对这个输入进行旋转，赋予其位置信息。因为论文中是通过旋转赋予位置编码，那么$x_q$和$q_m$的区别就是幅角**。

公式13：没啥好说的，query和key做内积

公式14：论文原文解释说，可以理解为“带有空位置信息的向量” (vectors with empty position information encoded) 。也就是说，$q$ 就是还没有旋转过的向量。

公式15：把公式12和公式13从二维平面推广到复数平面，使用极坐标的方式表示，前面的是模长，后面的幅角。

公式16：因为公式13，f_q和f_k做内积，会得到g的函数。现在要把极坐标的形式进行代入。因为极坐标的乘法是模长和模长相乘，幅角和幅角相乘。所以公式16的第一个式子没有问题。第二个式子因为是极坐标形式，做内积的时候，后面那一项需要取共轭，所以$\Theta_{k}(x_{k},n)$前面应该有个负号，但是实际论文中好像是反过来了。我也不知道为什么，但似乎不影响。正确的应该是：
$$\Theta_{q}(x_{q},m)-\Theta_{k}(x_{k},n)$$

公式17：和公式14差不多，是一个初始条件。假定位置是0，也就是旋转角度为0。

公式17应该和公式14放在一起，或者说公式17是公式14的进一步展开。q,k在二维平面上用极坐标表示，原始的角度是$\theta_q$和$\theta_k$。后面就是公式15的写法。从公式17的等式中，能得到两个推论：

$||q||=R_q(x_q,0)$
$\theta_q=\Theta_q(x_q,0)$

k也是同样的，这个推论在公式18会用到。这个推论，其实和公式19一样。不过不重要，只要能推导出来就行。

公式18：论文写的不清楚，而且可能有些笔误，按照我的方式来理解：

论文是要证明这个结论：

$$R_{q}(x_{q},m)R_{k}(x_{k},m)=\|q\||k\|$$

根据公式16可以得到：
$R_{q}(x_{q},m)R_{k}(x_{k},m)=R_{g}(x_{q},x_{k},0)$

再根据公式16，如果m和n都取0，也可以得到$R_{g}(x_{q},x_{k},0)$

所以：
$$R_{q}(x_{q},m)R_{k}(x_{k},m)=R_{g}(x_{q},x_{k},0)=R_{q}(x_{q},0)R_{k}(x_{k},0)$$

再根据公式17的推论。

就可以得到：
$$R_{q}(x_{q},m)R_{k}(x_{k},m)=\|q\||k\|$$

公式19：跳过，推论可以由公式17得到

过度公式：$$\Theta_k(x_k, m) - \Theta_q(x_q, m) = \theta_k - \theta_q$$


这个是由公式18b移动项得到的，这样看这个公式会好理解些：

$$\underbrace{\Theta_q(x_q, m) - \theta_q}_{\text{Query 的角度增量}} = \underbrace{\Theta_k(x_k, m) - \theta_k}_{\text{Key 的角度增量}}$$

这个公式得出一个结论，那就是Query 的角度增量和Key 的角度增量相同。这里关键是要理解这个差值相同，可以想象一个图辅助理解，假设q的初始角度是10，k的初始角度是20。那么q转动到35，k转动到45，两边的增量都是25。核心的点在于转了多少，而不是最后的角度是多少，而转了多少，是和m有关。

既然“角度的增量”只与 $m$ 有关，作者就定义了一个只与 $m$ 有关的函数 $\phi(m)$ 来表示这个增量：

$$\Theta(x, m) - \theta = \phi(m)$$

把 $\theta$ 移到等式右边，就得到了 公式 20 3：

$$\Theta_f(x_{\{q,k\}}, m) = \phi(m) + \theta_{\{q,k\}}$$


公式21：代入到公式16里面推一下就好，不难。

公式22：公式21的右边是一个常数，可以写为一个常数$\theta$。

$$\phi(m+1) - \phi(m) = \theta$$

我们要解出 $\phi(m)$ 是什么：

-   当 $m=0$ 时：$\phi(1) - \phi(0) = \theta \implies \phi(1) = \phi(0) + \theta$
-   当 $m=1$ 时：$\phi(2) - \phi(1) = \theta \implies \phi(2) = (\phi(0) + \theta) + \theta = \phi(0) + 2\theta$
-   当 $m=2$ 时：$\phi(3) - \phi(2) = \theta \implies \phi(3) = (\phi(0) + 2\theta) + \theta = \phi(0) + 3\theta$
-   推广到m:

$$\phi(m) = \phi(0) + m \cdot \theta$$

对应到公式22：
$$\phi(m) = m\theta + \gamma$$

$\theta$和$\gamma$都是常数，$\theta$非0。

公式23到公式25的推导没有什么太难的，就是公式25中的把$\gamma=0$要稍微解释下。

根据之前的推导，位置编码的相位函数（角度）是：

$$\text{总角度}(m) = \underbrace{\theta_q}_{\text{原始角度}} + \underbrace{m\theta}_{\text{位置旋转}} + \underbrace{\gamma}_{\text{初始偏移}}$$

如果m=0，也就是第一个token，会在原有角度上旋转一个角度$\gamma$。

但是相邻 token 的角度差异还是 $\theta$，并不影响模型捕捉相对位置依赖关系的能力（因为在计算 Attention 内积时，$\gamma$ 会在相减过程中被抵消掉），所以可以令 $\gamma=0$，看起来更加简洁。核心就是为了方便和简洁。


我们以论文中q和k的内积为例，在公式16中。

如果保留 $\gamma$，Query 和 Key 的角度分别是：

$\text{Angle}_q = \theta_q + m\theta + \gamma$

$\text{Angle}_k = \theta_k + n\theta + \gamma$

这两个部分对于公式16中的m和n，相减会直接把$\gamma$ 抵消。

