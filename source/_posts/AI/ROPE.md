---
title: ROPE
date: 2025/11/21
categories:
  - AI
tags:
  - ROPE
mathjax: true
abbrlink: 878b3a83
---

# 基础知识

## 三角函数

## 旋转矩阵 
https://zhuanlan.zhihu.com/p/183973440

XY坐标系中，向量OP旋转β角度到了OP'的位置：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-11-21_11-08-17.png)

根据三角函数关系，可以列出向量OP与OP'的坐标表示形式：

对比上面个两个式子，将第2个式子展开：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-cdf8b5fa36af46cdd4986cdbc3ec8d2a_r.png)


用矩阵形式重新表示为：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/v2-8803684cd287390b2b689c128d76563b_1440w.jpg)

中间的矩阵即二维旋转的旋转矩阵，坐标中的某一向量左乘该矩阵后，即得到这个向量旋转β角后的坐标。

# 论文核心逻辑

-   背景：Transformer 的自注意力机制（Self-Attention）计算的是 Query 和 Key 的内积。我们需要把位置信息 $m$ 和 $n$ 塞进 $q$ 和 $k$ 里。
-   目标：我们希望找到一种编码方式 $f(x, pos)$，使得 Query 和 Key 做内积（Attention Score）之后，结果只包含相对位置 $(m-n)$ 的信息。即：$\text{InnerProduct}(q_m, k_n)$ 应该等价于一个函数 $g(\dots, m-n)$。这个函数$g(\dots, m-n)$就是约束，经过位置编码后的 $q_m$ 和 $k_n$ 的内积，应该等于一个只与“相对距离 ($m-n$)”有关的函数 $g$：
$$\langle f_q(x_m, m), f_k(x_n, n) \rangle = g(x_m, x_n, m-n)$$
-   推导：
    -   先在 2维空间（复数域）推导，发现 旋转（Rotation） 完美满足上述条件。
    -   再推广到 $d$ 维空间，通过将向量切分为 $d/2$ 个子空间，分别旋转。
- 性质 (Properties, Sec 3.3): 这种旋转编码自带“远程衰减”特性，符合语言规律。

# 一些问题

## 问题1

论文公式（4）中的$f_q(x_m, m) = (W_q x_m) e^{im\theta}$。看起来有些不对，因为$W_q$一般是右乘词嵌入矩阵。右乘的时候，我们默认词嵌入矩阵$x_m$是一个行向量；而论文这里使用左乘，应该是将向量 $x$ 视为列向量。

## 问题2

公式2下面的公式，咋一看有些不对，但是仔细看，可以这样去理解。

$O_m$是第 $m$ 个位置的注意力分数输出，固定$m$，然后遍历整个序列$N$，所以这个公式是没有问题的。

# 公式解读

## 公式1

$q_m = f_q(x_m, m)$

$x_m$表示的是第$m$个token，而$m$表示这个token的位置索引。

$x_m$只是一个向量，没有位置。

ROPE需要拿着 $x_m$ （向量），根据 $m$ （位置整数）计算出一个旋转角度 $m\theta$，然后把向量转一下。如果不传 $m$，函数就不知道该转多少度。



## 公式18a

论文写的是$R_q(x_q,m) R_k(x_k,m) = R_q(x_q,x_k,0)$

应该是写错了，按照上面的公式，推导下来应该是：

$R_q(x_q,m) R_k(x_k,m) = R_g(x_q,x_k,0)$


按照论文的推导，可以得到：

$$R_g(x_q, x_k, 0) =  R_q(x_q, 0) \cdot R_k(x_k, 0) =||q|| \cdot ||k|| =$$

比较难理解的在于$R_q(x_q, 0) \cdot R_k(x_k, 0) =||q|| \cdot ||k||$ 


**可以这样理解：**

公式14，论文原文解释说，可以理解为“带有空位置信息的向量” (vectors with empty position information encoded) 。也就是说，$q$ 就是还没有旋转过的初始 Query 向量。

再看公式17，是在初始条件下（m=0）的情况下得到的。

$$\underbrace{q}_{\text{向量符号}} = \underbrace{||q|| e^{i\theta_q}}_{\text{向量的极坐标表示}} = \underbrace{R_q(x_q, 0) e^{i\Theta_q(x_q, 0)}}_{\text{函数的极坐标定义}}$$

前一个都好推导，因为这里是假设二维平面，$q$是一个二维向量，用极坐标写出来就好。
-   中间项 $||q|| e^{i\theta_q}$：这是复数 $q$ 的标准写法。
    -   $||q||$ 表示向量 $q$ 的模长（也就是长度）。
    -   $\theta_q$ 表示向量 $q$ 的原本角度。

后一部分，需要和公式15结合，把m=0代入就好。

-   右边项 $R_q(x_q, 0) e^{i\Theta_q(x_q, 0)}$：这是位置编码函数 $f_q$ 在 $m=0$ 时的写法。
    -  $R_q(x_q, 0)$ 是函数定义的模长部分。
    -   $\Theta_q(x_q, 0)$ 是函数定义的角度部分。

要想让公式成立，那么模长和模长要相等，辐角和辐角也相等。所以：

$$R_q(x_q, 0) = ||q||$$

$$R_k(x_k, 0) = ||k||$$

在这个基础上，回到公式18a就能得到：

$$R_g(x_q, x_k, 0) =  R_q(x_q, 0) \cdot R_k(x_k, 0) =||q|| \cdot ||k|| =$$


那么也就推出了，在m=n的条件下：
$$R_q(x_q, m)=R_k(x_k, 0)=||q|| \cdot ||k||$$

## 公式16第二行

这个公式是公式15和公式16的结合，理论上应该是下面这个才对：

$$\Theta_k(x_k, n) + \Theta_q(x_q, m) = \Theta_g(x_q, x_k, n-m)$$

而公式16第二行给出的是：

$$\Theta_k(x_k, n) - \Theta_q(x_q, m) = \Theta_g(x_q, x_k, n-m)$$

**关键在于为什么是减号而不是加号？**

原因在于$q_m$取了转置，在复平面上，就是共轭转置

## 维度和向量

向量的解释有很多种，空间中的箭头、有序的列表等都是正确的：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-09-26_11-09-38.png)

如果在平面当中，这就是一个二维向量。对于每个token来说，它是一个$d_model$维向量。**这里一定要和矩阵区分开来**。

在ROPE的论文里面，为了方便推导，先从d=2，也就是二维向量开始推导。然后推广到多维。


## 3.4小节解读

首先3.4小节明确了是二维的情况。按照公式1的说法：
$q_m = f_q(x_m, m)$

$x_m$表示的是第$m$个token，而$m$表示这个token的位置索引。

$x_m$只是一个向量，没有位置。

那么$x_q$是一个二维向量，而且论文把q进行转置，说明是按照列排序，而不是传统的行排序。行排序是w进行转置。

所以在3.4小节中，$x_q$是一个竖排向量，转置后就是横排，和$k_n$做内积，得到的是一个标量。

因为作者借助复数来推导，所以后面的内积是复数域的内积。

对于二维向量来说，内积就是对应部分乘积然后相加。

内积的符号：<>

a=[3,2]
b=[4,6]
<a,b> = 12+12=24

推广到复数域：a=3+2i; b=4+6i

如果直接计算内积：<a,b>=12+26i-12
因为内积是标量，所以会对计算结果取实部，但是结果显然不对，应该乘以后面那个复数的共轭。

<a,b> = Re[ab*]

------------------------

