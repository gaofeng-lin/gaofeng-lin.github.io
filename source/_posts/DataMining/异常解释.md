---
title: 异常解释
date: 2025/03/07
categories:
  - DataMining
tags:
  - 异常解释
  - 深度学习
mathjax: true
abbrlink: c2f86c81
---

# 常见的深度学习异常解释方法

## LIME(Local Interpretable Model-agnostic Explanations)

解释复杂机器学习模型（如深度学习）的局部可解释性方法

### 核心思想

**​局部性**：针对单个样本的预测结果，解释模型在该样本附近的决策逻辑（而非整个模型）。
**​模型无关性**：适用于任何黑箱模型（如神经网络、随机森林）。
**​简单代理模型**：在目标样本附近生成扰动数据，训练一个简单可解释的模型（如线性回归、决策树）来近似复杂模型的行为。


### 方法步骤

步骤1：选择目标样本
​输入：一张待解释的图片（例如包含狗的图片）。
​模型预测：模型输出类别概率（例如“狗”的概率为85%，“猫”为5%，“其他”为10%）。
​问题：模型为何认为这是“狗”？哪些像素区域起了关键作用？
​步骤2：生成扰动样本
​扰动方法：在目标样本周围生成大量扰动（修改后的样本）。
​对图像的扰动：将图像分割为多个“超像素”（Superpixel，相似像素的块），随机遮挡部分超像素生成新样本。
​示例：生成1000个扰动样本，每个样本随机保留或遮挡若干超像素（如图）。
​数学表示：每个扰动样本可表示为二值向量 x 
′
 ∈{0,1} 
d
 ，其中 d 是超像素数量，1表示保留该超像素，0表示遮挡。
​步骤3：获取黑箱模型的预测结果
​预测记录：将扰动样本输入原始模型，记录每个样本的预测概率。
​示例：遮挡狗头区域的扰动样本预测为“狗”的概率可能大幅下降。
​标签选择：通常选择黑箱模型预测概率最高的类别（如“狗”）作为解释目标。
​步骤4：计算样本相似性权重
​相似性度量：计算扰动样本与原始样本的相似性（距离），赋予权重。

​意义：与原始样本越相似的扰动样本，权重越高，对代理模型训练的影响越大。
​步骤5：训练可解释的代理模型
​代理模型选择：使用简单的可解释模型（如线性回归、决策树）拟合扰动样本的预测结果。
​优化目标：最小化加权损失函数：

步骤6：解释与可视化
​特征重要性：通过代理模型的参数（如线性模型的系数），标识重要特征。
​正权重：该特征支持当前预测（如保留“狗头”超像素会增加“狗”的概率）。
​负权重：该特征反对当前预测（如保留“草地”超像素可能降低“狗”的概率）。
​可视化：在原始图像上高亮重要超像素（如红色表示正向贡献，蓝色表示负向贡献）。

### 例子

假设用LIME解释一张“狗”图片的分类结果：

​超像素分割：将图片分割为50个超像素（如狗头、身体、草地背景等）。

​生成扰动样本：随机遮挡部分超像素，生成1000个扰动图像。

​黑箱预测：原始模型对遮挡狗头的扰动样本预测为“狗”的概率显著下降。

​代理模型训练：线性回归发现“狗头”超像素的权重最高（+0.8），背景权重为-0.1。

​可视化结果：在原始图片上高亮“狗头”区域，表明模型主要依赖该区域进行分类。

### 数学本质



### 优缺点

优点：
​模型无关：适用于任何黑箱模型。
​直观解释：通过热图或权重列表提供人类可理解的解释。
​灵活性：支持图像、文本、表格数据等多种输入类型。
​缺点：
​局部性限制：仅解释单个样本，无法反映模型全局行为。
​扰动生成依赖性：结果可能因超像素分割方式或扰动策略不同而变化。
​简单模型近似误差：线性模型可能无法完全拟合复杂模型的局部非线性行为。


## ANCHORS

一种生成 ​基于规则的局部解释 的方法，“为单个预测生成一组简洁的‘如果-那么’规则（锚点），覆盖满足这些规则的所有样本，且这些样本的预测结果高度一致。”​

​锚点（Anchor）​：一组特征条件（如“颜色=红色”且“尺寸>10cm”），满足这些条件的样本大概率被模型预测为同一类别。
​覆盖度（Coverage）​：锚点适用的样本在数据集中的比例。
​置信度（Precision）​：在锚点覆盖的样本中，模型预测结果与目标预测一致的占比（如95%置信度）。

### 核心思想

**(1) 规则的可信性与覆盖性**
ANCHORS生成的规则需满足：

​高置信度：覆盖的样本中，模型的预测结果应与目标预测高度一致（例如 ≥ 95%）。
​最大覆盖度：在保证置信度的前提下，规则应尽可能覆盖更多样本。
​**(2) 模型无关性**
与LIME类似，ANCHORS不依赖模型内部结构，适用于任何黑箱模型（如神经网络、随机森林）。

​**(3) 局部与全局的平衡**
虽然ANCHORS生成的是单个样本的局部规则，但这些规则可能覆盖一个局部区域（邻域），具有一定的泛化能力。


### 步骤

步骤1：初始化候选锚点
​输入：目标样本 x（例如一条文本“这部电影很棒”）。
​生成候选规则：
对离散特征：枚举可能的取值（如文本中的词、图像的超级像素是否保留）。
对连续特征：分箱处理（如将年龄分为“<18”、“18-60”、“>60”）。
​步骤2：蒙特卡洛采样与置信度验证
​蒙特卡洛采样：生成大量扰动样本，部分满足候选规则的条件。
​示例：对文本“这部电影很棒”，扰动可能包括删除部分词（如“电影”→“这部_很棒”）。
​置信度计算：在满足锚点规则的扰动样本中，统计模型预测结果一致的占比。
​数学表达：置信度 P= 满足锚点的样本数/预测一致的样本数
​
 。
​步骤3：贪心搜索最优锚点
​目标：找到最短（最简洁）且满足置信度阈值（如 P≥95%）的规则。
​搜索过程：
从空规则开始，逐步添加特征条件（如先添加“包含‘很棒’”，再添加“不包含‘无聊’”）。
每次选择能最大提升置信度的特征条件，直到满足阈值。
​步骤4：覆盖度优化
​目标：在保证置信度的前提下，扩大锚点覆盖的样本范围。
​方法：尝试删除冗余条件或合并相似规则。


### 数学本质
ANCHORS的数学目标是找到一个锚点 A，使得：

$P(f(x^{\prime})=f(x)\mid A(x^{\prime})=1)\geq\tau$

$A(x^{\prime})=1$表示样本$x^{\prime}$满足锚点规则，$\tau$表示用户定义的置信度阈值

### 例子

​目标样本：X光片显示肺部阴影。
​模型预测：肺炎（概率92%）。
​ANCHORS规则： \text{如果左上肺叶有≥3cm高密度阴影且无钙化点，则预测为肺炎（置信度96%，覆盖度8%）}
​解释：规则帮助医生验证模型是否关注了正确的病理特征。。

### 数学本质



### 优缺点

优点
​高可信度：通过统计验证保证规则置信度。
​可解释性强：规则形式符合人类直觉（如“如果-那么”逻辑）。
​支持多种数据类型：文本、图像、表格数据均可处理。
​缺点
​计算复杂度高：蒙特卡洛采样和贪心搜索在大特征空间下效率低。
​覆盖度受限：高置信度规则可能覆盖样本较少，难以泛化。
​规则可能冗余：例如“包含词A且包含词B”可能不如“包含词A或B”简洁。


## SHAP(SHapley Additive exPlanations)
基于博弈论的Shapley值，为每个特征分配对预测的贡献值，解释全局或局部行为。

“公平分配每个特征对模型预测的贡献值，满足一致性、可加性和对称性等公理。”​

​Shapley值：源自合作博弈论，衡量每个玩家（特征）对总收益（预测值）的边际贡献。
​可加性：所有特征的SHAP值之和等于预测值与平均预测值的差值。
​全局与局部解释：可解释单个预测（局部）或整个模型（全局）。

### 核心思想

​公平分配：特征贡献值等于其在所有可能特征子集中边际贡献的平均值。
​加性一致性：所有特征的SHAP值之和等于预测值与平均预测值的差值。

### 步骤

步骤1：计算Shapley值
对每个特征 i，计算其Shapley值：



### 例子

​房价预测：特征“面积=100㎡”的SHAP值为+50k，表示它使预测房价比平均值高50k。
​图像分类：SHAP热图显示狗耳朵区域的贡献值最高。

### 数学本质



### 优缺点

优点
​理论坚实：唯一满足公平性公理的解释方法。
​全局与局部统一：可解释单个预测和整体模型行为。
​可视化丰富：支持多种解释图表（瀑布图、蜂群图、依赖图）。
​缺点
​计算成本高：精确计算Shapley值在大特征量下不可行（需近似方法）。
​基线依赖：特征贡献值依赖基线选择（如均值或零输入），不同基线导致不同解释。
​忽略高阶交互：默认计算一阶贡献，复杂交互需额外分析