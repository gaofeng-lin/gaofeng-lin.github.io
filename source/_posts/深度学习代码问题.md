---
title: 深度学习代码问题
date: 2023/11/24
categories:
  - AI
tags:
  - tensor
mathjax: true
abbrlink: 53791
---


## 遇到的问题

### loss.backward()耗时严重
loss.backward()主要是计算梯度，为后面更新参数做准备，如果这一步耗时严重，要么是网络结构复杂，要么是**计算图非常大**。如果某个tensor的requires.grad属于被开启，那么就会为其分配一个计算图。

**背景：**
```
pde_data = torch.from_numpy(pd.read_csv(self.config.pde_data_path).values).float().to(self.device)

self.pde_Xs = [pde_data[:, i:i+1].requires_grad_() for i in range(self.config.X_dim)]
```
上面这段代码是读取数据后，然后开启requires_grad_()，但是它是对所有的数据都开启requires_grad_()，即使后面batch只使用了很小一部分数据，但是loss.backward()还是会计算这一部分，导致很慢。

**正确的做法：**
在取出batch的时候才为这些数据开启requires_grad_()


### tensor必须在同一个设备上面

有时候需要把numpy转为tensor，再转回来。

可能会遇到不在同一个设备上面，需要阻断反向传播等。
下面给一个简单的例子：

```
// 把numpy转为tensor，再换为32位的（网络接受的是32位），在移动到同一个device上
predict = net(torch.from_numpy(XYT).float().to(config.device))

// tensor转换为numpy，除了要先放到cpu上，还需要阻断反向传播，requires_grad为false
predict = predict.cpu().detach().numpy()
```

## 深度学习代码解析
### 自定义激活函数并调用
```
import torch
import torch.nn as nn

# 定义一个自定义的激活函数
class CustomActivation(nn.Module):
    def __init__(self, a_init_value):
        super().__init__()
        self.a = nn.Parameter(torch.tensor(a_init_value))

    def forward(self, x):
        return 1.0 / (1.0 + torch.exp(-self.a * x))


# 定义一个网络层，其中使用了自定义的激活函数
class MyLayer(nn.Module):
    def __init__(self, input_size, output_size):
        super(MyLayer, self).__init__()
        self.linear = nn.Linear(input_size, output_size)
        self.custom_activation = CustomActivation(a_init_value=0.1)

    def forward(self, x):
        x = self.linear(x)
        x = self.custom_activation(x)
        return x

# 使用自定义的网络层
my_layer = MyLayer(10, 20)

# 随机生成一个输入数据
input_data = torch.randn(5, 10)

# 使用自定义的网络层进行前向传播
output_data = my_layer(input_data)
```
首先，我们从torch.nn模块中导入Module类，然后定义了一个新的类CustomActivation，这个类继承了Module类，所以它也是一个PyTorch的网络模块。我们在CustomActivation类的初始化函数__init__中定义了一个可以训练的参数self.a。这个参数是用nn.Parameter函数从输入的初始化值a_init_value创建的，它会自动添加到模块的参数列表中，所以在优化过程中，优化器会自动更新这个参数。

在CustomActivation类的forward方法中，我们定义了这个自定义激活函数的具体操作。当我们用这个模块处理输入数据时，PyTorch会自动调用这个方法。在这个方法中，我们先用torch.exp函数计算-self.a * x的指数，然后再用1.0 / (1.0 + ...)计算出这个自定义激活函数的输出。

接下来，我们定义了一个新的网络层MyLayer。这个网络层有一个线性层self.linear和一个自定义激活函数self.custom_activation。在这个网络层的forward方法中，我们先用线性层处理输入数据，然后再用自定义激活函数处理线性层的输出。

在这段代码的最后部分，我们创建了一个MyLayer的实例my_layer，然后用这个实例处理了一个随机生成的输入数据input_data。我们先用torch.randn函数生成了一个形状为(5, 10)的随机张量，然后把这个张量作为输入数据传递给了my_layer。在这个过程中，PyTorch会自动调用my_layer的forward方法，计算出网络层的输出。

总的来说，这段代码主要展示了如何在PyTorch中自定义一个激活函数，并把这个激活函数用在一个网络层中。在这个过程中，我们用到了PyTorch的Module类、Parameter类、Linear类等关键功能。

在my_layer = MyLayer(10, 20)中，10和20分别是神经网络层（线性层）的输入维度和输出维度。

输入维度10意味着每个输入样本应有10个特征。
输出维度20意味着该层将每个输入样本转换为具有20个特征的输出。
input_data = torch.randn(5, 10)中的5和10分别代表批次大小（batch size）和特征数。其中：