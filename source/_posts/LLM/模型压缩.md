---
title: 模型压缩
date: 2025/03/26
categories:
  - LLM
tags:
  - 知识蒸馏
mathjax: true
---

# 知识蒸馏

知识蒸馏是对模型的能力进行迁移，将一个大模型（教师模型）在经可能保证其预测效果的前提下压缩为一个小模型（学生模型）。

知识蒸馏的核心是：知识、蒸馏算法、师生架构

按照知识来说，可以分为：基于Logits/响应、基于特征、基于关系。

蒸馏的方式也有：离线蒸馏、在线蒸馏、自蒸馏

下面以**离线蒸馏、基于Logits**为样例，解释知识蒸馏的流程。

## 训练阶段

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-03-26_14-28-15.png)

教师模型和学生模型的输入都是相同的。

分类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。但是知识蒸馏过程中会进行一个“温度挑战”输出概率分布：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-03-26_14-50-59.png)

就是每个概率除以"T"。

学生模型的输出也通过相同的温度缩放，以匹配教师模型的分布。

学生模型的LOSS由两部分组成，一部分是蒸馏损失，另一部分是监督损失。总LOSS是两部分的加权和，参数$\lambda$控制知识迁移强度。

## 为什么需要温度参数T

**能增强学生模型的泛化能力**

上面这句话是相比于**使用完全相同的模型结构和训练数据只使用Hard-target的训练方法得到的模型**

原因是：

1. 传统的Hard-target的训练方式，所有的负标签都会被平等对待。**Soft-target给Student模型带来的信息量要大于Hard-target，并且Soft-target分布的熵相对高时，其Soft-target蕴含的知识就更丰富。**
2. 使用Soft-target训练时，梯度的方差会更小，训练时可以使用更大的学习率，所需要的样本也更少。

T越大，Softmax的输出就会越加平缓，信息熵越大。


## 推理阶段

学生模型在推理时移除温度参数（就是令T=1），直接输出原始概率分布。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-03-26_15-09-13.png)