---
title: 时空预测
date: 2023/12/25
categories:
  - research_papers
tags:
  - computer vision
  - Spatiotemporal predictive learning
mathjax: true
abbrlink: d7c97432
---

# SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning

## 模型架构


![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-44-51.png)

三部分均由CNN组成

其中Translator有两种选择：

IncepU：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-43.png)

gSTA：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-50.png)

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-56.png)


# Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**

## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

- 准确的时空预测学习可以使气候变化[74, 77]、人类运动预测[91, 107]、交通流量预测[18, 97]和表征学习[39, 71]等领域的广泛实际应用受益。时空预测学习的意义主要在于其探索物理世界中空间相关性和时间演化的潜力。
- 此外，时空预测学习的自监督性质非常符合人类的学习方式，无需大量标记数据。海量视频可提供丰富的视觉信息，使时空预测学习成为一种生成性预训练策略[35, 60]，用于特征表征学习，以完成各种下游视觉监督任务。

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

- 

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**


# TIMER-XL: LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**

- 这个工作是在“Timer: Generative Pre-trained Transformers Are Large Time Series Models”的工作基础上进行优化的，是同一批人做的。
- 实验是在A100上面进行的。


## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

Transformer被广泛应用于时间序列预测，

不论是

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

- 现有的**时序**Transformer通常只能运行（输入/输出）百个量级的时序tokens，而**自然语言和视觉**的Transformer可以学习数千到数百万个tokens之间的依赖关系。

为什么现有的**时序**Transformer通常只能运行（输入/输出）百个量级的时序tokens？

这篇论文在说明这一点的时候引用了一篇论文（A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.）
在引文中的附录A.1.2中提到：“The default look-back windows for different baseline models could be different. For Transformerbased models, the default look-back window is L = 96; and for DLinear, the default look-back window is L = 336. The reason of this difference is that Transformer-based baselines are easy to overfit when look-back window is long while DLinear tend to underfit. ”


**大概的意思就是窗口过大，基于Transformer的模型容易过拟合，而基于Dlinear的模型容易欠拟合。但是这篇引文没有分析为什么容易过拟合**




-  对于单变量预测，短语境输入导致对全局趋势的学习不足，难以解决现实世界时间序列中的非平稳性问题（Hyndman，2018）。对于多变量预测，越来越多的研究证明了明确捕捉通道内和通道间依赖关系的有效性（Zhang & Yan，2022；Liu et al.，2023；2024a），这凸显了扩展上下文长度以涵盖相互关联的时间序列的实际紧迫性。
   -  这里解释下对于多变量预测来说，为什么捕捉通道内和通道间依赖关系和上下文长度有关。
   -  对于通道内依赖：指同一变量在不同时间点的依赖关系（例如，温度序列中的季节性变化）。如果上下文长度过短，模型可能无法捕捉到长期趋势（如年周期、周周期），导致预测失败。例如，预测电力负荷时，若上下文仅包含几天数据，模型可能无法学习到夏季高温的长期规律。
   -  对于通道间依赖：指不同变量之间的相互作用（例如，温度与电力负荷的关系）。如果变量间的依赖关系是跨时间的（如某地降雨量在几周后影响另一地的农业产量），模型必须看到足够长的历史数据才能学习这种延迟关联。例如，在气候预测中，风速和气压的长期关联可能需要数月的数据来建模。

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

- 提出了多变量下一个标记预测和统一的时间序列预测，通过扩大上下文来强化 Transformers，从而做出信息完整的预测。
- 介绍了 TimeAttention，这是一种为多维时间序列量身定制的新型因果自关注，可通过位置感知促进序列内和序列间建模，并保持 Transformers 的因果性和可扩展性。


- TimeAttention: 旨在捕获所有变量内部和变量之间的因果patches依赖关系

## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

从三个方面对Timer-XL进行评价：
- 作为特定任务的预测器(Supervised training as a task-specific forecaster):
  - 这个方面关注的是模型在​**​有监督训练​**​下，在​**​各个独立、具体的预测任务**​​上的性能。这正是通过第4.1、4.2、4.3节分别在​**​单变量预测、多变量预测、协变量预测​**​这三个经典且不同的任务场景上做实验来体现的。每一节都是在某个特定任务上“从头训练”一个模型，并证明Timer-XL在该任务上的优越性。
- 作为零样本预测器(Large-scale pre-training as a zero-shot forecaster)
  - 这个方面关注模型的​**​泛化能力和潜力**​​。通过第4.4节的大规模​​预训练​​实验，作者展示了Timer-XL一旦经过海量数据预训练后，无需在下游任务上微调（即零样本），就能在未曾见过的数据集和任务上取得良好效果，这使其有成为“基础模型”的潜力。
- 评估TimeAttention的有效性和模型效率(Assessing the effectiveness and efficiency)
  - 在第4.5节，作者专门分析了其核心创新**​​TimeAttention机制的有效性**​​（如可视化）和模型的**​​计算效率**​​。同时，在前三节的实验里，与其它模型的对比本身也包含了对TimeAttention有效性的评估（例如，对比通道独立与通道依赖的模型）。

### 4.1

 


### 4.2 多变量预测实验

输入长度是672，输出长度是96。将预测结果作为输入进行迭代预测，预测四个长度{96，192，336，720}


### 评估核心组件与效率








## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

- 第一章第四段，Multivariate Next Token Prediction：指在时间序列预测任务中，同时建模多个变量之间的依赖关系，并基于当前和历史观测值，联合预测下一个时间步的多个变量值。
- 第三章Approach。对时序变量token的定义：
  - 一个序列长度是TP，一个时序token被定义为P个连续的时间点，那么每个patch token的长度就是T。
  - 序列长度=token数量*token长度。而P就是token长度。

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**





