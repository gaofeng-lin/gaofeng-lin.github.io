---
title: 时空预测
date: 2023/12/25
categories:
  - research_papers
tags:
  - computer vision
  - Spatiotemporal predictive learning
mathjax: true
abbrlink: d7c97432
---

# SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning

## 模型架构


![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-44-51.png)

三部分均由CNN组成

其中Translator有两种选择：

IncepU：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-43.png)

gSTA：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-50.png)

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-56.png)


# Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**

## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

- 准确的时空预测学习可以使气候变化[74, 77]、人类运动预测[91, 107]、交通流量预测[18, 97]和表征学习[39, 71]等领域的广泛实际应用受益。时空预测学习的意义主要在于其探索物理世界中空间相关性和时间演化的潜力。
- 此外，时空预测学习的自监督性质非常符合人类的学习方式，无需大量标记数据。海量视频可提供丰富的视觉信息，使时空预测学习成为一种生成性预训练策略[35, 60]，用于特征表征学习，以完成各种下游视觉监督任务。

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

- 

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**


# TIMER-XL: LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**

- 这个工作是在“Timer: Generative Pre-trained Transformers Are Large Time Series Models”的工作基础上进行优化的，是同一批人做的。
- 实验是在A100上面进行的。


## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

Transformer被广泛应用于时间序列预测，

不论是

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

- 现有的**时序**Transformer通常只能运行（输入/输出）百个量级的时序tokens，而**自然语言和视觉**的Transformer可以学习数千到数百万个tokens之间的依赖关系。

为什么现有的**时序**Transformer通常只能运行（输入/输出）百个量级的时序tokens？

这篇论文在说明这一点的时候引用了一篇论文（A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.）
在引文中的附录A.1.2中提到：“The default look-back windows for different baseline models could be different. For Transformerbased models, the default look-back window is L = 96; and for DLinear, the default look-back window is L = 336. The reason of this difference is that Transformer-based baselines are easy to overfit when look-back window is long while DLinear tend to underfit. ”


**大概的意思就是窗口过大，基于Transformer的模型容易过拟合，而基于Dlinear的模型容易欠拟合。但是这篇引文没有分析为什么容易过拟合**




-  对于单变量预测，短语境输入导致对全局趋势的学习不足，难以解决现实世界时间序列中的非平稳性问题（Hyndman，2018）。对于多变量预测，越来越多的研究证明了明确捕捉通道内和通道间依赖关系的有效性（Zhang & Yan，2022；Liu et al.，2023；2024a），这凸显了扩展上下文长度以涵盖相互关联的时间序列的实际紧迫性。
   -  这里解释下对于多变量预测来说，为什么捕捉通道内和通道间依赖关系和上下文长度有关。
   -  对于通道内依赖：指同一变量在不同时间点的依赖关系（例如，温度序列中的季节性变化）。如果上下文长度过短，模型可能无法捕捉到长期趋势（如年周期、周周期），导致预测失败。例如，预测电力负荷时，若上下文仅包含几天数据，模型可能无法学习到夏季高温的长期规律。
   -  对于通道间依赖：指不同变量之间的相互作用（例如，温度与电力负荷的关系）。如果变量间的依赖关系是跨时间的（如某地降雨量在几周后影响另一地的农业产量），模型必须看到足够长的历史数据才能学习这种延迟关联。例如，在气候预测中，风速和气压的长期关联可能需要数月的数据来建模。

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

- 提出了多变量下一个标记预测和统一的时间序列预测，通过扩大上下文来强化 Transformers，从而做出信息完整的预测。
- 介绍了 TimeAttention，这是一种为多维时间序列量身定制的新型因果自关注，可通过位置感知促进序列内和序列间建模，并保持 Transformers 的因果性和可扩展性。


- TimeAttention: 旨在捕获所有变量内部和变量之间的因果patches依赖关系

## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

从三个方面对Timer-XL进行评价：
- 作为特定任务的预测器(Supervised training as a task-specific forecaster):
  - 这个方面关注的是模型在​**​有监督训练​**​下，在​**​各个独立、具体的预测任务**​​上的性能。这正是通过第4.1、4.2、4.3节分别在​**​单变量预测、多变量预测、协变量预测​**​这三个经典且不同的任务场景上做实验来体现的。每一节都是在某个特定任务上“从头训练”一个模型，并证明Timer-XL在该任务上的优越性。
- 作为零样本预测器(Large-scale pre-training as a zero-shot forecaster)
  - 这个方面关注模型的​**​泛化能力和潜力**​​。通过第4.4节的大规模​​预训练​​实验，作者展示了Timer-XL一旦经过海量数据预训练后，无需在下游任务上微调（即零样本），就能在未曾见过的数据集和任务上取得良好效果，这使其有成为“基础模型”的潜力。
- 评估TimeAttention的有效性和模型效率(Assessing the effectiveness and efficiency)
  - 在第4.5节，作者专门分析了其核心创新**​​TimeAttention机制的有效性**​​（如可视化）和模型的**​​计算效率**​​。同时，在前三节的实验里，与其它模型的对比本身也包含了对TimeAttention有效性的评估（例如，对比通道独立与通道依赖的模型）。

### 4.1 单变量预测




### 4.2 多变量预测实验

输入长度是672，输出长度是96。将预测结果作为输入进行迭代预测，预测四个长度{96，192，336，720}


### 评估核心组件与效率








## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

- 第一章第四段，Multivariate Next Token Prediction：指在时间序列预测任务中，同时建模多个变量之间的依赖关系，并基于当前和历史观测值，联合预测下一个时间步的多个变量值。
- 第三章Approach。对时序变量token的定义：
  - 一个序列长度是TP，一个时序token被定义为P个连续的时间点，那么每个patch token的长度就是T。
  - 序列长度=token数量*token长度。而P就是token长度。

--------


## 实验复现

因为需要复现实验，需要先统计下做了哪些实验，每个实验是如何设置的。

代码网址：https://github.com/thuml/OpenLTM
数据集描述：https://github.com/thuml/OpenLTM/blob/main/figures/datasets.png

**数据集描述**：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-10-12_09-00-18.png)

**实验配置**：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-10-12_09-01-31.png)

### 单变量时序预测

图3实验

#### 实验步骤
  - 预测的步长都是96。输入的数据长度不断增加，一共有5个不同的类型输入长度：1day(24步)、4day（96步）、1week（168步）、1month（672步）、4month（2880步）。我准备复制ETTh1这个数据集的结果。那么一共就是要进行5次实验，每次要修改输入的步长。每次的epoch为10次。
#### 单变量时序预测采用多变量时序数据集要怎么处理
  - ~~不过这里有个问题（已解决，在下一个点里面有提到），那就是论文在4.1中的Setups提到"Although these datasets are originally multivariate, they aim to be predicted in a univariate approach with the implementation of channel independence."。这里我有些没看懂，因为图3的实验都是多变量数据集，但是实验却是单变量预测，那么选用哪个变量进行训练呢？论文没有说。但在附录B.3中提到“We adopt channel independence from Nie et al. (2022) in univariate time series forecasting.”。这个可能就是处理的方法，后面需要去看下这个论文是怎么做的。~~ 。关于上面的问题，有了回答。参考：https://zhuanlan.zhihu.com/p/602332939 。 所谓的channel independence就是对多变量分别进行预测，再将预测的结果拼接起来，这就相当于每个维度是独立的。
#### 实验结果

代码里面连续三次验证集loss没有下降就早停。

- **输入步长24(1day)**：
  - 第一次实验：epoch 5早停；mse:0.4135052263736725, mae:0.4306889474391937
  - 第二次实验：epoch 5早停；mse:0.4135052263736725, mae:0.4306889474391937
  - 第三次实验：epoch 5早停；mse:0.4135052263736725, mae:0.4306889474391937

**平均效果：mse:0.4135052263736725, mae:0.4306889474391937**

- 如果把data从MultivariateDatasetBenchmark换为UnivariateDatasetBenchmark，**mse:0.37723585963249207, mae:0.3987869918346405**

- **输入步长96(4day)**：
  - 第一次实验：mse:0.3673262298107147, mae:0.3994017541408539
  - 第二次实验：mse:0.3673262298107147, mae:0.3994017541408539
  - 第三次实验：mse:0.3673262298107147, mae:0.3994017541408539

**平均效果：mse:0.3673262298107147, mae:0.3994017541408539**

- 如果把data从MultivariateDatasetBenchmark换为UnivariateDatasetBenchmark，**mse:0.3619115948677063, mae:0.3970280587673187**

- **输入步长168(1week)**：
  - 第一次实验：epoch 9早停；mse:0.3750412166118622, mae:0.40709444880485535

后面两次结果也一样。


**平均效果：mse:0.3750412166118622, mae:0.40709444880485535**

替换后
mse:0.36265239119529724, mae:0.40084296464920044

- **输入步长672(1month)**：
  - 第一次实验：epoch 6早停；mse:0.5115599036216736, mae:0.5127909183502197
  - 第二次实验：epoch 6早停；mse:0.5115599036216736, mae:0.5127909183502197
  - 第三次实验：epoch 6早停；mse:0.5115599036216736, mae:0.5127909183502197

**平均效果：mse:0.5115599036216736, mae:0.5127909183502197**

- patch_size参数的默认值是16，在论文里面，这个值的设置是96，替换为96后，结果是一摸一样。epoch6早停，mse,mae一样。

- 如果把data从MultivariateDatasetBenchmark换为UnivariateDatasetBenchmark，在epoch4早停，**mse:0.42560669779777527, mae:0.4508516490459442**

- **输入步长2880**：



### 多变量时序预测--table4
#### 输入672步长，输出96。数据集ETTh1
  - 论文结果：mse:0.409, mae:0.430

~~  第一次结果：mse:0.7172967791557312, mae:0.6388883590698242。第二次一样。第三次结果一样~~

  ~~复现结果比较差，mse提高了75.3%；mae提高了48.6%~~


**上面的结果有问题，因为是预测4个不同步长的结果，然后取平均，但是代码没有把4个结果取平均，而是单独输出，所以还需要自己计算下。计算后的结果：**

mse:0.576475；mae:0.5523

~~目前我自己写了一个脚本，运行timer-xl模型，数据集是etth1，但是没法运行。把模型换成moirai就可以运行。我看了下核心的问题，是在exp_forecast.py文件中的```outputs = self.model(batch_x, batch_x_mark, batch_y_mark)```这段代码，三个维度分别是：
batch_x shape: torch.Size([32, 672, 7])
batch_x_mark shape: torch.Size([32, 672, 1])
batch_y_mark shape: torch.Size([32, 672, 1])如果模型是timer-xl。输出outputs shape: torch.Size([32, 672, 7])如果是moirai。outputs shape: torch.Size([32, 96, 7])而在后面的```loss = criterion(outputs, batch_y)```就会报错。因为batch_y的维度是torch.Size([32, 96, 7])，而timer-xl输出的outputs结果是([32, 672, 7])~~。**这个问题已经解决**。

#### 输入672步长，输出96。数据集ECL

论文结果：mse:0.155, mae:0.246

~~复现结果：mse:0.17138800024986267, mae:0.27118179202079773~~

4个长度的预测结果
mse:0.14015242457389832, mae:0.24011272192001343
mse:0.1517903357744217, mae:0.2513769268989563
mse:0.15890070796012878, mae:0.2604312002658844
mse:0.17138800024986267, mae:0.27118179202079773

#### 输入672步长，输出96。数据集Traffic

#### 输入672步长，输出96。数据集Weather

#### 输入672步长，输出96。数据集Solar-Energy

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**



# Are Transformers Effective for Time Series Forecasting?

## Summary

- LTSF（long-term time series forecasting）
- DMS（direct multi-step）
- IMS（iterated multi-step）

2023-AAAI

作者质疑了LTST-Transformer在长时间时序预测中的有效性，通过与LTSF-linear的方法进行对比来验证猜想。

Transformer的核心是多头注意力机制，该机制在提取长序列中元素之间的语义关联方面有非常强的能力。但是注意力机制是具有permutation-invariant，会破坏时序中的order。为例验证这个想法，将LTSF-linear和LTST-Transformer方法进行比较，并进行以下实验：
- **注意力机制是否对LTSF有效**
  - **步骤**：逐渐将Informer中的注意力层转换为线性层。
  - **结论**：随着模型越来越简化，性能也在不断提高。对这些模块的必要性提出质疑。
- **现有的LTST-Transformer能否较好的保留时间序列**
  - **步骤**：通过两种方式破坏输入序列的原始顺序：​​Shuf. (Random Shuffle)​​: 将整个输入序列的顺序完全随机打乱、Half-Ex. (Half Exchange)​​: 将输入序列的前半部分和后半部分直接交换。这个实验应该是先把模型训练好，然后输入三种不同的序列（原始、shuf、half-ex），观察和原始的相比性能下降多少。如果原始的数据是周期性很强的数据，那么打乱后，性能应该会下降很多。并且周期性越强的数据，性能下降越多。
  - **结论**：
    - Exchange数据集（周期性差）：LTST-Transformer影响非常小；LTST-Linear效果很差，shuf后性能下降很明显。
    - ETTh1数据集（周期性一般）：FEDformer和Autoformer有“time series inductive bias”模块，导致性能下降很大；而Informer没有这个，性能下降的比较少。
    - 论文说因为LTST-Linear的平均下降率要高于LTST-Transformer，所以LTST-Transformer的方法不能很好的保留时序。这个结论我觉得可以这样理解，因为是比较原始和打乱的差异，时序保留的越好，那么打乱后，效果就会越差。而LTST-Transformer的下降率要比LTST-Linear的低，说明它反而没有保留好时序。
- **不同embedding策略效果如何**
  - 要去了解下三个former的embedding策略，位置的和时间的。
- **训练数据大小对LTST-Transformer的影响**
  - 可以去看看清华那篇timer-xl，看看它的测试长度是多少，作者在这个地方的结论是弱结论。
- **LTST-Transformer效率真的提升了吗**
  - 从表8可以看出来，和经典Transformer相比，大多数Transformer的变体在推理时间和参数上都差不多，甚至变差了。因为后面的这些变体引入了更多的设计，使得实际成本变得更高。

论文作者提到的核心贡献不在于提出了一个线性模型，而在于抛出了一个重要问题（Transformer在时序预测中真的有效吗），展示了令人惊讶的对比，并从不同角度证明了为什么 LTSF-Transformers 并不像这些著作中声称的那样有效。


## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

-  Transformer的核心是多头注意力机制，该机制在提取长序列中元素之间的语义关联方面有非常强的能力。
-  但是注意力机制是具有“排列不变性”的。排列不变性是指：在语义丰富的NLP任务中，如果我们把一个句子中部分词位置置换，再喂入Transformer学习，对point-wise自关注机制学习其语义信息并不会有太大的影响。这便是排列不变性。
-  排列不变性让模型只关注语义信息，而忽略时序顺序信息。但在时序数值数据中，order信息是非常重要的，并且很多时序数据（如股价、电价）是缺乏语义信息的，因此我们希望模型更关注连续点集，而不是离散点集，但实际上用point-wise的自注意力机制，应具有排列不变性，会导致过拟合时序噪声，而非真正时序信息，导致泛化性能差。

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

-  证明Transformer没有学习到时序信息

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

-  作者假设长时间预测仅仅对具有相对清晰的趋势和周期性的时序数据有用（introduction-5para），而线性模型已经可以提取此类信息。所以选用名为LTSF的模型为baseline，直接预测未来时间序列。



## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

### 对比方法
- 基础base-line:
  ![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/3761a8710387b91893d7563013a2b7a8-image.png)
  数学表达式为：$\hat{X_i}=WX_i$。权重矩阵W的维度是T*L，T是预测窗口大小，L是输入窗口大小。

  预测时间步1：$\hat{X_{i}^{1}}=W_{1,1}X_{i}^{1}+W_{1,2}X_{i}^{2}+...+W_{1,L}X_{i}^{L}$

  预测时间步2：$\hat{X_{i}^{2}}=W_{2,1}X_{i}^{1}+W_{2,2}X_{i}^{2}+...+W_{2,L}X_{i}^{L}$
  **只要矩阵 W中每一行的权重值不同，即使输入序列$X_i$相同，计算出的每个未来时间步的预测值也是不同的​​。**
  
- 为了更好的处理不同领域的数据，在基础的LTSF-Linear基础上引入两个变体：
    - DLinear: 将Autoformer和FEDformer中的”Decomposition“策略和线性层结合。首先通过移动平均核将原始数据输入分解为趋势成分和剩余（季节性）成分。然后，对每个分量应用两个单层线性层，将两个特征相加得出最终预测结果。
    - NLinear: NLinear 首先用序列的最后一个值减去输入值。然后，输入经过一个线性层，再将减去的部分加回来，最后进行预测。NLinear 中的减法和加法是对输入序列的简单归一化。
  - 四种Tranformer的变体：FEDformer、Autoformer、Informer、Pyraformer
  - 原始的DMS方法：重复look-back窗口中的最后一个值

### 注意力机制是否对LTSF有效
  - 逐渐将Informer中的内容转换为线性层。将注意力层转换为线性层，得到Att-Linear；摒弃Informer中的其他辅助设计（如FFN），只留下嵌入层和线性层，得到Embed+Linear。最后将模型简化为线性层。
  ![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-10-02_08-09-58.png)
  **结论：随着模型越来越简化，性能也在不断提高。对这些模块的必要性提出质疑。**

### 现有的LTSF-Transformer能较好的保留时间序列吗？
  - 该实验旨在测试模型是否真正理解和依赖输入序列的时间顺序。它通过两种方式破坏输入序列的原始顺序：​​Shuf. (Random Shuffle)​​: 将整个输入序列的顺序完全随机打乱、Half-Ex. (Half Exchange)​​: 将输入序列的前半部分和后半部分直接交换。
  - 如果模型严重依赖于正确的时间顺序，那么在这种顺序被破坏后，其性能（即MSE）应该会显著下降。
  - “Average Drop”这一列量化了模型在顺序被破坏后，性能​​变差的程度​​。这个指标要分为Shuf和Half-Ex两部分来看。我们以FEDformer在ETTh1上的性能来看，一共有4个不同预测长度的结果，分别是0.753、0.730、0.736、0.720，这四个值分别和对应的Ori结果计算变化率，以第一个为例，(0.753-0.376)/0.376。然后把四个结果加起来除以4，就得到了“Average Drop”。**这个结果正的越多，表面结果下降的越厉害；负的越多，表面性能还有提升。**
  ![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-10-02_08-42-38.png)
  - 结论：
    - 对于汇率数据集。所有的LTSF-Transformer方法性能变化很小，而LTSf-Linear的性能却大幅下降。这表明，具有不同位置嵌入和时间嵌入的LTSF-Transforme保留的时间关系相当有限，容易对有噪声的金融数据产生过拟合，而简单的 LTSF-Linear 可以自然地对顺序进行建模，并以较少的参数避免过拟合。
    - 对于ETTh1数据集，FEDformer和Autoformer因为引入了”time series inductive bias“，使得能提取更具有周期性数据的特征。因此，在Shuf设置下，两个模型的性能下降很多。而Informer没有这种temporal inductive bias，效果反而下降的少。

### 不同嵌入策略的效果如何？

- 如果没有位置嵌入（wo/Pos.）如果没有时间戳嵌入（wo/Temp.），随着预测长度的增加，Informer 的性能也会逐渐下降。**由于 Informer 对每个token只使用一个时间步长**，因此有必要在每个token中引入时间信息。
-  FEDformer 和 Autoformer 不在每个token中使用单一的时间步长，而是**输入一系列时间戳来嵌入时间信息**。因此，在没有固定位置嵌入的情况下，它们可以实现相当甚至更好的性能。
-  如果没有时间戳嵌入，Autoformer 的性能就会因为全局时间信息的丢失而迅速下降。
-  由于 FEDformer 中**提出了频率增强模块来引入时间归纳偏差**，因此它在移除任何位置/时间戳嵌入后受到的影响较小。

### 训练数据的大小是现有基于Transformer的LTSF方法的限制因素吗？

- 在训练数据减少的情况下，预测误差通常较小。这可能是因为与较长但不完整的数据规模相比，全年数据能保持更清晰的时间特征。
- 虽然我们不能得出结论说应该使用更少的数据进行训练，但这表明训练数据规模并不是限制性原因。

### 效率真的是最重要的因素吗？

- 现有的基于Transformer的LTSF方法声称原始Transformer的时间复杂度都是$O^2$，尽管他们进行了优化，理论上最高优化到了$O$，但是还不清楚，实际的推理时间和内存消耗是否也优化了。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-10-08_07-55-57.png)

从表8可以看出来，和经典Transformer相比，大多数Transformer的变体在推理时间和参数上都差不多，甚至变差了。因为后面的这些变体引入了更多的设计，使得实际成本变得更高。


## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**



## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**



- 知乎评论有人提到，这篇论文是由漏洞的，找的实验用的数据集基本看不出周期性，EXchang数据真就完全没有啥周期性。ETT有点，但是不多。如果换成PEMS04这样的分布漂移不严重的数据，这些工作效果最好的就是Informer。（Are Transformers Effective for Time Series Forecasting? - 马东什么的文章 - 知乎
https://zhuanlan.zhihu.com/p/569194246）

- 有一篇论文对这篇文章的论点进行了回应，发现了在时间序列上高效使用transformer的方式，效果超越DLinear和其他formers。见知乎评论：Transformer是否适合用于做非NLP领域的时间序列预测问题？ - 科研汪老徐的回答 - 知乎
https://www.zhihu.com/question/493821601/answer/2506641761 。 论文名称：A Time Series is Worth 64 Words: Long-term Forecasting with Transformers



**可以去验证下是不是这样**

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**

- AutoFormer、Informer、FEDformer、Pyraformer。这篇论文提到，Informer输入一系列时间戳来嵌入时间信息，在没有固定位置嵌入的情况下，效果也不错；AutoFormer提出了频率增强模块来引入时间归纳偏差，因此它在移除任何位置/时间戳嵌入后受到的影响较小。



# Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**

2021-NeurIPS

## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

- **动机和必要性**：
  - 时序预测广泛应用于能源消耗、交通和经济规划、天气和疾病传播预测等领域
  - 迫切的需求就是将预测时间延长到更远的未来，这对于长期规划和预警来说意义重大。

- **现有研究局限**：
  - 直接从长期时间序列中发现时间依赖关系是不可靠的，因为依赖关系可能会被纠缠（entangled）的时间模式所掩盖。
  - 典型的Transformer-based方法计算复杂度高，之前有方法将注意力改为稀疏注意力，但是使用了point-wise representation aggregation。会因为稀疏的point-wise而牺牲信息利用率，从而造成时间序列长期预测的瓶颈。

- **解决这些局限的研究挑战**：
  - 为了对错综复杂的时间模式进行推理，我们尝试采用分解的思想，这是时间序列分析中的一种标准方法。它可以用来处理复杂的时间序列，提取出更多可预测的成分。由于未来是未知的，Decomposition只能用作过去序列的预处理。这种常见的用法限制了分解的能力，**忽略了被分解成分之间未来潜在的相互作用**。


- **当前方法的出发点和解决思路**：
  - 基于时序分解方法，并试图超越分解的预处理用法。使深度预测模型具备渐进分解的内在能力。此外，分解还能揭示纠缠不清的时间模式，突出时间序列的固有特性[15]。得益于此，我们尝试利用序列的周期性翻新自我关注中的点-线联系。我们观察到，各周期中处于同一相位的子序列往往呈现出相似的时间过程。因此，我们尝试根据序列周期性得出的过程相似性来构建序列级连接。

- **当前方法的主要技术方案**：
  - Auto-Correlation：在序列层面发现依赖关系并进行信息聚合。根据序列的周期性发现子序列的相似性，并从基础周期中汇总相似的子序列。对于长度为 L 的序列，这种系列机制的复杂度为 O(L log L)，并通过将点表示聚合扩展到子序列级别。我们的机制超越了以往的自相关系列，可同时提高计算效率和信息利用率。

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？这部分可以从Introduction最后一段的contributions中去找，贡献对应的就是解决了什么问题）**

- Decomposition这种预处理方法受限于历史序列的简单分解效应，忽略了未来长期序列基本模式之间的层次互动关系。

- 典型的Transformer-based方法计算复杂度高，之前有方法将注意力改为稀疏注意力，但是使用了point-wise representation aggregation。会因为稀疏的point-wise而牺牲信息利用率，从而造成时间序列长期预测的瓶颈。


## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**


- 针对问题1：从一个新的渐进维度引入分解思想。我们的 Autoformer 将分解作为深度模型的内部模块，可以在整个预测过程中逐步分解隐藏序列，包括过去序列和预测的中间结果。
- 针对问题2：Auto-Correlation：在序列层面发现依赖关系并进行信息聚合。根据序列的周期性发现子序列的相似性，并从基础周期中汇总相似的子序列。对于长度为 L 的序列，这种系列机制的复杂度为 O(L log L)，并通过将点表示聚合扩展到子序列级别。我们的机制超越了以往的自相关系列，可同时提高计算效率和信息利用率。

### Decomposition Architecture:
  - Series decomposition block:
    - 这部分的操作主要是将原始的数据进行分解，得到趋势-周期和季节两部分数据。具体的操作见公式1，这个过程被概括为${\cal X}_{\mathrm{s}},{\cal X}_{\mathrm{t}}=\mathrm{Serics}\mathrm{Deconp}({\cal X})$。对应图1中的“Series Decomp”模块
  -  Model inputs:
     -  encoder输入：过去$I$个时间步，$\mathcal{X}_{\mathrm{en}}\ \in\mathbb{R}^{I\times d}$
     -  decoder输入：包含趋势-周期和季节性两部分呢输入，两部分输入的维度是待定，都是$({\frac{I}{2}}+O)*d$
     -  每次输入
  - Encoder：
    - $\mathcal{S}_{\mathrm{en}}^{l,i},i\in\mathrm{~\{l,2\}}$：因为Encoder部分有两个Series Decomp模块，这里面的上标$i$表示第几个Series Decomp模块的输出。
  - Decoder：
    - 分为两部分：趋势-周期成分的累计结构；季节成分的叠加自相关结构。
    - 每个Decoder部分都包含inner Auto-Correlation和encoder-decoder Auto-Correlation。其中encoder-decoder Auto-Correlation应该就是Decoder部分中间的Auto-Correlation模块，这个模块的k和v是来自Encoder。分别完善预测和利用过去的季节性信息。
    - Decoder整体流程分为三个部分，两个Auto-Correlation+Series Decomp，再接一个Feed Forward+Series Decomp。 

### Auto-Correlation Mechanism:

- **1. 设计动机**：
  传统Transformer模型采用​​点对点（Point-wise）​​ 的自注意力机制，这种方法在长序列预测中存在明显局限：
  - 计算效率低
  - 信息利用瓶颈：点对点的关联方式难以有效捕捉时间序列中固有的​​子序列级（Sub-series level）​​ 的周期性模式。

  Auto-Correlation机制的工作流程主要包括两个步骤：​**​基于周期的依赖关系发现​​和​​时延聚合**​​。


- **2. 基于周期的依赖关系**：
这一步的目标是自动找到时间序列中隐藏的周期长度。具体做法是计算序列的自相关函数（Autocorrelation）。
  - **自相关函数**：对于序列 {X_t}，其与自身滞后τ个时间步的序列 {X{t-τ}} 的相似性，通过自相关函数 R{XX}(τ) 来衡量。R_{XX}(τ) 的值越高，说明该滞后时间τ所代表的周期长度可能性越大。
  - **快速计算**：论文利用​​维纳-辛钦定理​​，通过快速傅里叶变换（FFT）来高效计算所有可能滞后（τ from 1 to L）的自相关值，将复杂度降至 O(L log L)。
  - **选择关键周期**：算法并非使用所有周期，而是选取自相关值最高的k个时延τ₁, τ₂, ..., τₖ（其中 k = ⌊c × log L⌋，c为超参数）。这些τ值即为模型识别出的最主要周期长度，其对应的R(τ)值作为聚合时的权重（置信度）。

- **3. 时延聚合**：
这一步的目的是基于发现的周期，将不同周期中相似相位（即相同位置）的子序列信息进行聚合。
  - **滚动操作（Roll）**：对于每一个选定的关键时延τ_i，对值（Value）序列执行Roll(X, τ_i)操作。这个操作将序列整体向后滚动τ_i个时间步，使得当前时刻的相位与τ_i个时间步前的相位对齐。
  - **加权聚合​​**：最后，将所有经过滚动对齐的子序列，根据其自相关值R(τ_i)（经过Softmax归一化）进行加权求和，得到最终的聚合表示。

- **4. 高效计算**：
在效率上，得益于FFT和仅聚合O(log L)个子序列，整个Auto-Correlation机制的时间复杂度和空间复杂度均保持在 ​​O(L log L)​​，使其能够高效处理长序列。

- **5. 与Self-Attention家族的对比**：
Figure 3 清晰地展示了Auto-Correlation与各种自注意力机制的本质区别：
  - ​​（a）全注意力​​：所有点之间进行两两连接，计算开销大。
  - ​​（b）稀疏注意力​​（如Informer, Reformer）：基于某种度量选择部分关键点进行连接，仍是点对点。
  - ​​（c）Log稀疏注意力​​：按指数间隔选择点进行连接。
  - ​​（d）Auto-Correlation​​：**​​根本性地将依赖发现和信息聚合从点级提升到了子序列级​**​。它基于序列的周期性，建立的是​​序列与序列（Series-wise）​​ 之间的连接。

这种根本性的改变，使得Auto-Correlation在长时序预测任务中，​​同时兼顾了计算效率和信息利用的有效性​​，突破了传统点对点自注意力的瓶颈。

- **6. 自相关计算参考资料**
相关的概念可以参考这两个网站：https://zhuanlan.zhihu.com/p/583185155、https://zhuanlan.zhihu.com/p/430503143


## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**


### 消融实验

- **Table3**
  - 目的：验证论文所提出架构的有效性
  - 操作：**把论文提出的progressive architecture架构应用到其他模型，验证相比原模型是否有提高**。这个就是表3中的Ours；在应用了架构后，分别训练两个模型，一个模型接收pre-decomposed seasonal的输入，并对其预测。另一个接收trend-cyclical components作为输出输出，最后将两个结果以某种方式合并。

- **Table4**
  - 目的：验证论文所提出注意力机制的有效性
  - 操作：将Autoformer中的Auto-Correlation分别替换为其他注意力机制，例如：Full Attention、LogSparse Attention、LSH Att、ProbSparse Att。各种输入长度的效果都是最好的，并且部分方法还会内存溢出。

### 模型分析

这部分几乎没看懂，不知道它画的图到底想表达什么。等今天下午或明天再重新看下。

## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

- 文中的idea是基于Decomposition方法，后面可以去了解下，参考文献是[1, 27]
- 第三章节中的Encoder部分，提到“the encoder focuses on the seasonal part modeling”。没有明白为什么重点是季节部分，而不是趋势-周期？
- 4.3小节中的模型分析。我不太理解Seasonal Part和Trend-cyclical具体应该是什么样？比如说，一个时序数据被分解为seasonal和trend，那么应该是什么样，后面可以让ai给我写一个简单程序看一下。

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**












