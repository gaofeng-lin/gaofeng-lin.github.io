---
title: 时空预测
date: 2023/12/25
categories:
  - research_papers
tags:
  - computer vision
  - Spatiotemporal predictive learning
abbrlink: d7c97432
---

# SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning

## 模型架构


![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-44-51.png)

三部分均由CNN组成

其中Translator有两种选择：

IncepU：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-43.png)

gSTA：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-50.png)

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-56.png)


# TIMER-XL: LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**

- 这个工作是在“Timer: Generative Pre-trained Transformers Are Large Time Series Models”的工作基础上进行优化的，是同一批人做的。
- 实验是在A100上面进行的。


## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

Transformer被广泛应用于时间序列预测，

不论是

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

- 现有的时序Transformer通常只能运行（输入/输出）百个量级的时序tokens，而自然语言和视觉的Transformer可以学习数千到数百万个tokens之间的依赖关系。

为什么现有的时序Transformer通常只能运行（输入/输出）百个量级的时序tokens？

这篇论文在说明这一点的时候引用了一篇论文（A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.）
在引文中的附录A.1.2中提到：“The default look-back windows for different baseline models could be different. For Transformerbased models, the default look-back window is L = 96; and for DLinear, the default look-back window is L = 336. The reason of this difference is that Transformer-based baselines are easy to overfit when look-back window is long while DLinear tend to underfit. ”


**大概的意思就是窗口过大，基于Transformer的模型容易过拟合，而基于Dlinear的模型容易欠拟合。但是这篇引文没有分析为什么容易过拟合**




-  对于单变量预测，短语境输入导致对全局趋势的学习不足，难以解决现实世界时间序列中的非平稳性问题（Hyndman，2018）。对于多变量预测，越来越多的研究证明了明确捕捉通道内和通道间依赖关系的有效性（Zhang & Yan，2022；Liu et al.，2023；2024a），这凸显了扩展上下文长度以涵盖相互关联的时间序列的实际紧迫性。
   -  这里解释下对于多变量预测来说，为什么捕捉通道内和通道间依赖关系和上下文长度有关。
   -  对于通道内依赖：指同一变量在不同时间点的依赖关系（例如，温度序列中的季节性变化）。如果上下文长度过短，模型可能无法捕捉到长期趋势（如年周期、周周期），导致预测失败。例如，预测电力负荷时，若上下文仅包含几天数据，模型可能无法学习到夏季高温的长期规律。
   -  对于通道间依赖：指不同变量之间的相互作用（例如，温度与电力负荷的关系）。如果变量间的依赖关系是跨时间的（如某地降雨量在几周后影响另一地的农业产量），模型必须看到足够长的历史数据才能学习这种延迟关联。例如，在气候预测中，风速和气压的长期关联可能需要数月的数据来建模。

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

- 提出了多变量下一个标记预测和统一的时间序列预测，通过扩大上下文来强化 Transformers，从而做出信息完整的预测。
- 介绍了 TimeAttention，这是一种为多维时间序列量身定制的新型因果自关注，可通过位置感知促进序列内和序列间建模，并保持 Transformers 的因果性和可扩展性。


- TimeAttention: 旨在捕获所有变量内部和变量之间的因果patches依赖关系

## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

- 第一章第四段，Multivariate Next Token Prediction：指在时间序列预测任务中，同时建模多个变量之间的依赖关系，并基于当前和历史观测值，联合预测下一个时间步的多个变量值。

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**


# 互联网服务场景下基于机器学习的KPI异常检测综述

KPI(Key performance indicator, 关键性能指标)。KPI数据是经过专家筛选的能够高度反映当前服务软硬件及业务状态的关键指标，其数据结构为数值型，具备丰富的时序 信息且易于存储。


## KPI异常检测技术框架

### KPI监控与预处理

互联网服务KPI监控体系 主要概括为3个层次：**基础资源**、**应用性能**及**用户体验**，其中以**基础资源监控（通常将其KPI定义为机器KPI）和应用性能监控（通常将其KPI定义为服务KPI） 在KPI监控中最为常见**. 表4所示为各层次KPI监控 范围介绍及举例. 常用基础资源监控工具包括Zabbix 等，常用应用性能监控工具包括SkyWalking等。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-09-18_10-58-20.png)

### 异常检测模型构建与训练

异常检测模型构建一般又分为**机器学习模型选择和机器学习模型优化设计**2部分。模型的选择主要取决于KPI中的依赖模式. 不同服务存在不同的依赖模式。而模 型的优化设计则更多考虑影响检测技术性能（包括 准确性、鲁棒性、实时性、可解释性等）的多重因素， 如KPI噪声分布、模型计算复杂度等。

## KPI异常检测技术的模型选择动机及分类


