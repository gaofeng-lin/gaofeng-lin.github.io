---
title: 时空预测
date: 2023/12/25
categories:
  - research_papers
tags:
  - computer vision
  - Spatiotemporal predictive learning
mathjax: true
abbrlink: d7c97432
---

# SimVP: Towards Simple yet Powerful Spatiotemporal Predictive Learning

## 模型架构


![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-44-51.png)

三部分均由CNN组成

其中Translator有两种选择：

IncepU：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-43.png)

gSTA：
![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-50.png)

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2023-12-25_09-50-56.png)


# Temporal Attention Unit: Towards Efficient Spatiotemporal Predictive Learning
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**

## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

- 准确的时空预测学习可以使气候变化[74, 77]、人类运动预测[91, 107]、交通流量预测[18, 97]和表征学习[39, 71]等领域的广泛实际应用受益。时空预测学习的意义主要在于其探索物理世界中空间相关性和时间演化的潜力。
- 此外，时空预测学习的自监督性质非常符合人类的学习方式，无需大量标记数据。海量视频可提供丰富的视觉信息，使时空预测学习成为一种生成性预训练策略[35, 60]，用于特征表征学习，以完成各种下游视觉监督任务。

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

- 

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**


# TIMER-XL: LONG-CONTEXT TRANSFORMERS FOR UNIFIED TIME SERIES FORECASTING
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**

- 这个工作是在“Timer: Generative Pre-trained Transformers Are Large Time Series Models”的工作基础上进行优化的，是同一批人做的。
- 实验是在A100上面进行的。


## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

Transformer被广泛应用于时间序列预测，

不论是

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

- 现有的**时序**Transformer通常只能运行（输入/输出）百个量级的时序tokens，而**自然语言和视觉**的Transformer可以学习数千到数百万个tokens之间的依赖关系。

为什么现有的**时序**Transformer通常只能运行（输入/输出）百个量级的时序tokens？

这篇论文在说明这一点的时候引用了一篇论文（A time series is worth 64 words: Long-term forecasting with transformers. arXiv preprint arXiv:2211.14730, 2022.）
在引文中的附录A.1.2中提到：“The default look-back windows for different baseline models could be different. For Transformerbased models, the default look-back window is L = 96; and for DLinear, the default look-back window is L = 336. The reason of this difference is that Transformer-based baselines are easy to overfit when look-back window is long while DLinear tend to underfit. ”


**大概的意思就是窗口过大，基于Transformer的模型容易过拟合，而基于Dlinear的模型容易欠拟合。但是这篇引文没有分析为什么容易过拟合**




-  对于单变量预测，短语境输入导致对全局趋势的学习不足，难以解决现实世界时间序列中的非平稳性问题（Hyndman，2018）。对于多变量预测，越来越多的研究证明了明确捕捉通道内和通道间依赖关系的有效性（Zhang & Yan，2022；Liu et al.，2023；2024a），这凸显了扩展上下文长度以涵盖相互关联的时间序列的实际紧迫性。
   -  这里解释下对于多变量预测来说，为什么捕捉通道内和通道间依赖关系和上下文长度有关。
   -  对于通道内依赖：指同一变量在不同时间点的依赖关系（例如，温度序列中的季节性变化）。如果上下文长度过短，模型可能无法捕捉到长期趋势（如年周期、周周期），导致预测失败。例如，预测电力负荷时，若上下文仅包含几天数据，模型可能无法学习到夏季高温的长期规律。
   -  对于通道间依赖：指不同变量之间的相互作用（例如，温度与电力负荷的关系）。如果变量间的依赖关系是跨时间的（如某地降雨量在几周后影响另一地的农业产量），模型必须看到足够长的历史数据才能学习这种延迟关联。例如，在气候预测中，风速和气压的长期关联可能需要数月的数据来建模。

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

- 提出了多变量下一个标记预测和统一的时间序列预测，通过扩大上下文来强化 Transformers，从而做出信息完整的预测。
- 介绍了 TimeAttention，这是一种为多维时间序列量身定制的新型因果自关注，可通过位置感知促进序列内和序列间建模，并保持 Transformers 的因果性和可扩展性。


- TimeAttention: 旨在捕获所有变量内部和变量之间的因果patches依赖关系

## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

从三个方面对Timer-XL进行评价：
- 作为特定任务的预测器(Supervised training as a task-specific forecaster):
  - 这个方面关注的是模型在​**​有监督训练​**​下，在​**​各个独立、具体的预测任务**​​上的性能。这正是通过第4.1、4.2、4.3节分别在​**​单变量预测、多变量预测、协变量预测​**​这三个经典且不同的任务场景上做实验来体现的。每一节都是在某个特定任务上“从头训练”一个模型，并证明Timer-XL在该任务上的优越性。
- 作为零样本预测器(Large-scale pre-training as a zero-shot forecaster)
  - 这个方面关注模型的​**​泛化能力和潜力**​​。通过第4.4节的大规模​​预训练​​实验，作者展示了Timer-XL一旦经过海量数据预训练后，无需在下游任务上微调（即零样本），就能在未曾见过的数据集和任务上取得良好效果，这使其有成为“基础模型”的潜力。
- 评估TimeAttention的有效性和模型效率(Assessing the effectiveness and efficiency)
  - 在第4.5节，作者专门分析了其核心创新**​​TimeAttention机制的有效性**​​（如可视化）和模型的**​​计算效率**​​。同时，在前三节的实验里，与其它模型的对比本身也包含了对TimeAttention有效性的评估（例如，对比通道独立与通道依赖的模型）。

### 4.1

 


### 4.2 多变量预测实验

输入长度是672，输出长度是96。将预测结果作为输入进行迭代预测，预测四个长度{96，192，336，720}


### 评估核心组件与效率








## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

- 第一章第四段，Multivariate Next Token Prediction：指在时间序列预测任务中，同时建模多个变量之间的依赖关系，并基于当前和历史观测值，联合预测下一个时间步的多个变量值。
- 第三章Approach。对时序变量token的定义：
  - 一个序列长度是TP，一个时序token被定义为P个连续的时间点，那么每个patch token的长度就是T。
  - 序列长度=token数量*token长度。而P就是token长度。

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**



# Are Transformers Effective for Time Series Forecasting?

## Summary

- LTSF（long-term time series forecasting）
- DMS（direct multi-step）
- IMS（iterated multi-step）

Transformer的核心是多头注意力机制，该机制在提取长序列中元素之间的语义关联方面有非常强的能力。但是注意力机制是具有permutation-invariant，会破坏时序中的order。为例验证这个想法，将LTSF-linear和LTST-Transformer方法进行比较，并进行以下实验：
- **注意力机制是否对LTSF有效**
  - **步骤**：逐渐将Informer中的注意力层转换为线性层。
  - **结论**：随着模型越来越简化，性能也在不断提高。对这些模块的必要性提出质疑。
- 现有的LTST-Transformer能否保留时间或空间特征
  - **步骤**：通过两种方式破坏输入序列的原始顺序：​​Shuf. (Random Shuffle)​​: 将整个输入序列的顺序完全随机打乱、Half-Ex. (Half Exchange)​​: 将输入序列的前半部分和后半部分直接交换。
  - **结论**：
- 不同embedding策略效果如何
- 训练数据大小对LTST-Transformer的影响
- LTST-Transformer效率真的提升了吗


## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

-  Transformer的核心是多头注意力机制，该机制在提取长序列中元素之间的语义关联方面有非常强的能力。
-  但是注意力机制是具有“排列不变性”的。排列不变性是指：在语义丰富的NLP任务中，如果我们把一个句子中部分词位置置换，再喂入Transformer学习，对point-wise自关注机制学习其语义信息并不会有太大的影响。这便是排列不变性。
-  排列不变性让模型只关注语义信息，而忽略时序顺序信息。但在时序数值数据中，order信息是非常重要的，并且很多时序数据（如股价、电价）是缺乏语义信息的，因此我们希望模型更关注连续点集，而不是离散点集，但实际上用point-wise的自注意力机制，应具有排列不变性，会导致过拟合时序噪声，而非真正时序信息，导致泛化性能差。

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**

-  证明Transformer没有学习到时序信息

## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

-  作者假设长时间预测仅仅对具有相对清晰的趋势和周期性的时序数据有用（introduction-5para），而线性模型已经可以提取此类信息。所以选用名为LTSF的模型为baseline，直接预测未来时间序列。



## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

### 对比方法
- 基础base-line:
  ![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/3761a8710387b91893d7563013a2b7a8-image.png)
  数学表达式为：$\hat{X_i}=WX_i$。权重矩阵W的维度是T*L，T是预测窗口大小，L是输入窗口大小。

  预测时间步1：$\hat{X_{i}^{1}}=W_{1,1}X_{i}^{1}+W_{1,2}X_{i}^{2}+...+W_{1,L}X_{i}^{L}$

  预测时间步2：$\hat{X_{i}^{2}}=W_{2,1}X_{i}^{1}+W_{2,2}X_{i}^{2}+...+W_{2,L}X_{i}^{L}$
  **只要矩阵 W中每一行的权重值不同，即使输入序列$X_i$相同，计算出的每个未来时间步的预测值也是不同的​​。**
  
- 为了更好的处理不同领域的数据，在基础的LTSF-Linear基础上引入两个变体：
    - DLinear: 将Autoformer和FEDformer中的”Decomposition“策略和线性层结合。首先通过移动平均核将原始数据输入分解为趋势成分和剩余（季节性）成分。然后，对每个分量应用两个单层线性层，将两个特征相加得出最终预测结果。
    - NLinear: NLinear 首先用序列的最后一个值减去输入值。然后，输入经过一个线性层，再将减去的部分加回来，最后进行预测。NLinear 中的减法和加法是对输入序列的简单归一化。
  - 四种Tranformer的变体：FEDformer、Autoformer、Informer、Pyraformer
  - 原始的DMS方法：重复look-back窗口中的最后一个值

### 注意力机制是否对LTSF有效
  - 逐渐将Informer中的内容转换为线性层。将注意力层转换为线性层，得到Att-Linear；摒弃Informer中的其他辅助设计（如FFN），只留下嵌入层和线性层，得到Embed+Linear。最后将模型简化为线性层。
  ![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-10-02_08-09-58.png)
  **结论：随着模型越来越简化，性能也在不断提高。对这些模块的必要性提出质疑。**

### 现有的LTSF-Transformer能较好的保留时间序列特征吗？
  - 该实验旨在测试模型是否真正理解和依赖输入序列的时间顺序。它通过两种方式破坏输入序列的原始顺序：​​Shuf. (Random Shuffle)​​: 将整个输入序列的顺序完全随机打乱、Half-Ex. (Half Exchange)​​: 将输入序列的前半部分和后半部分直接交换。
  - 如果模型严重依赖于正确的时间顺序，那么在这种顺序被破坏后，其性能（即MSE）应该会显著下降。
  - “Average Drop”这一列量化了模型在顺序被破坏后，性能​​变差的程度​​。这个指标要分为Shuf和Half-Ex两部分来看。我们以FEDformer在ETTh1上的性能来看，一共有4个不同预测长度的结果，分别是0.753、0.730、0.736、0.720，这四个值分别和对应的Ori结果计算变化率，以第一个为例，(0.753-0.376)/0.376。然后把四个结果加起来除以4，就得到了“Average Drop”。**这个结果正的越多，表面结果下降的越厉害；负的越多，表面性能还有提升。**
  ![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-10-02_08-42-38.png)
  - 结论：
    - 对于汇率数据集。所有的LTSF-Transformer方法性能变化很小，而LTSf-Linear的性能却大幅下降。这表明，具有不同位置嵌入和时间嵌入的LTSF-Transforme保留的时间关系相当有限，容易对有噪声的金融数据产生过拟合，而简单的 LTSF-Linear 可以自然地对顺序进行建模，并以较少的参数避免过拟合。
    - 对于ETTh1数据集，FEDformer和Autoformer因为引入了”time series inductive bias“，使得能提取更具有周期性数据的特征。因此，在Shuf设置下，两个模型的性能下降很多。而Informer没有这种temporal inductive bias，效果反而下降的少。

### 不同嵌入策略的效果如何？

- 如果没有位置嵌入（wo/Pos.）如果没有时间戳嵌入（wo/Temp.），随着预测长度的增加，Informer 的性能也会逐渐下降。**由于 Informer 对每个token只使用一个时间步长**，因此有必要在每个token中引入时间信息。
-  FEDformer 和 Autoformer 不在每个token中使用单一的时间步长，而是**输入一系列时间戳来嵌入时间信息**。因此，在没有固定位置嵌入的情况下，它们可以实现相当甚至更好的性能。
-  如果没有时间戳嵌入，Autoformer 的性能就会因为全局时间信息的丢失而迅速下降。
-  由于 FEDformer 中**提出了频率增强模块来引入时间归纳偏差**，因此它在移除任何位置/时间戳嵌入后受到的影响较小。

### 训练数据的大小是现有基于Transformer的LTSF方法的限制因素吗？

- 在训练数据减少的情况下，预测误差通常较小。这可能是因为与较长但不完整的数据规模相比，全年数据能保持更清晰的时间特征。
- 虽然我们不能得出结论说应该使用更少的数据进行训练，但这表明训练数据规模并不是限制性原因。

### 效率真的是最重要的因素吗？

- 现有的基于Transformer的LTSF方法声称原始Transformer的时间复杂度都是$O^2$，尽管他们进行了优化，理论上最高优化到了$O$，但是还不清楚，实际的推理时间和内存消耗是否也优化了。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img1/Snipaste_2025-10-08_07-55-57.png)

从表8可以看出来，和经典Transformer相比，大多数Transformer的变体在推理时间和参数上都差不多，甚至变差了。因为后面的这些变体引入了更多的设计，使得实际成本变得更高。


## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**



## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

Are Transformers Effective for Time Series Forecasting? - 马东什么的文章 - 知乎
https://zhuanlan.zhihu.com/p/569194246

知乎评论有人提到，这篇论文是由漏洞的，找的实验用的数据集基本看不出周期性，如果换成PEMS04这样的分布漂移不严重的数据，这些工作效果最好的就是Informer。

**可以去验证下是不是这样**

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**

- AutoFormer、Informer、FEDformer、Pyraformer。这篇论文提到，Informer输入一系列时间戳来嵌入时间信息，在没有固定位置嵌入的情况下，效果也不错；AutoFormer提出了频率增强模块来引入时间归纳偏差，因此它在移除任何位置/时间戳嵌入后受到的影响较小。



# Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series Forecasting
## Summary
**（写完笔记之后最后填，概述文章的内容，以后查阅笔记的时候先看这一段。注：写文章summary切记需要通过自己的思考，用自己的语言描述。忌讳直接Ctrl + c原文。）**


## Background 
**（研究的背景，帮助你理解研究的动机和必要性，包括行业现状和之前研究的局限性。）**

## Problem Statement
**（问题陈述：问题作者需要解决的问题是什么？）**



## Method(s)
**（作者解决问题的方法/算法是什么？是否基于前人的方法？基于了哪些？）**

## Evaluation
**（作者如何评估自己的方法？实验的setup是什么样的？感兴趣实验数据和结果有哪些？有没有问题或者可以借鉴的地方？）**

## Conclusion
**（作者给出了哪些结论？哪些是strong conclusions, 哪些又是weak的conclusions。即作者并没有通过实验提供evidence，只在discussion中提到；或实验的数据并没有给出充分的evidence?）**

## Notes(optional) 
**（不在以上列表中，但需要特别记录的笔记。）**

## References(optional) 
**（列出相关性高的文献，以便之后可以继续track下去。）**












