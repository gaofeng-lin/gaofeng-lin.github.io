---
title: 深度学习
date: 2023/2/23
categories:
  - 深度学习
tags:
  - 深度学习
  - pytorch
mathjax: true
abbrlink: 48513
---

## 预备知识

### 名词解释

regression:回归

training data set = training set:训练数据集/训练集

sample = data point = data instance:样本/数据点/数据样本

label = target:标签/目标

feature = covariate:特性/协变量

translation:平移

gradient descent:梯度下降

minibatch stochastic gradient descent:小批量随机梯度下降

batch size:批量大小

hyperparameter tuning:调参



### 矩阵计算

在深度学习相关的资料里面，标量就表示一个数，向量是由多个数组成的。

常规的导数求导没有什么难度，现在将导数扩展到向量，会出现四种情况：

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/Snipaste_2023-03-02_22-49-16.png)

y为标量或向量；x为标量或向量

1、标量求导就不说了，高中常识；
2、y是标量，x是向量的情况。实际上就是y=f(x1,x2,...,xn)的意思。拿y=f(x1,x2)为例解释，有一个三维坐标轴体系，水平面的横轴和竖轴分别是x1、x2，立面上的轴是y，水平面上任意一个点(x1,x2)都对应y轴上的一个点，很明显这就是一个面，**因此他的导数是一个向量**，所以结果是横着写的。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/Snipaste_2023-03-02_22-54-05.png)

3、y是向量，x是标量的情况。这实际上就是【y1,y2,...,yn】=【f(x1),f(x2),...,f(xn)】，对x求导就是求出y=yi时那一个点上的斜率，**结果是标量**，所以结果是竖着写的。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/Snipaste_2023-03-02_22-55-43.png)

4、y、x都是向量的情况。根据上面描述，求导实际上就是求出了y=yi时，那一个平面形状边缘上的**向量**，因此是横着写的。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/Snipaste_2023-03-02_22-56-14.png)

------------------------------------

关于动手学习深度学习，自动微分章节里面2.5.1例子的理解[原链接](https://zh.d2l.ai/chapter_preliminaries/autograd.html)

我们对函数$y=2x^Tx$关于列向量x求导，假设变量x为 $x=[0,1,2,3]$

可以容易的得到y是标量，且值为28.
参考上面提到的四种情况，求导结果应该为向量。

可以把$y=2x^Tx$看作是$y=2x^2$，求导后为$4x$，那么带入$x=[0,1,2,3]$，最后的结果为$[0,4,8,12]$

还有另外一种理解方式，把向量$x$里面的值用${x_1},{x_2}$代替，那么$y=2{x_1^2}+2{x_2^2}+2{x_3^2}+2{x_4^2}$，然后再对每个分量进行求导，即可得到梯度。$[4{x_1},4{x_2},4{x_3},4{x_4}]$，把${x_1},{x_2}$的值带入，得到最终的结果$[0,4,8,12]$

同样的，对于该小节下面的例子
```
x.grad.zero_()  //  x梯度清零，x=[0,1,2,3]
y=x.sum()     // 按上面的方法，y=x1+x2+x3+x4
y.backward()  
x.grad
```
x梯度清零，$x=[0,1,2,3]$

按上面的方法，$y=x1+x2+x3+x4$

$\frac{\partial y}{\partial x} = [1,1,1,1] $


### 损失函数和梯度下降的关系

以线性回归为例，模型为：$y=wx+b$。
其中$w$,$b$是我们要求的参数，深度学习大多数时候就是要把参数求出来

损失函数：为了量化目标的实际值与预测值之间的差距。以平方损失函数为例，带入样本就可以得到差距。损失函数值越小，说明效果越好。**我们就是要找到使损失函数值最小的那组参数**

**梯度下降就是让我们找到那组参数的优化算法**

下面举一个梯度下降法的使用例子。[例子来源](https://www.cnblogs.com/pinard/p/5970503.html)

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/QQ截图20230316215219.png)

上图中步骤4稍微说明下，是单独对每个变量求偏导数后得到的，这样结果就是一个标量而不是向量。具体的过程看下图。

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/Snipaste_2023-03-16_21-55-24.png)

**当梯度下降的距离小于给定的值，就停止计算，得到的参数值就是最终的结果。**


### 熵、信息熵、相对熵、KL散度、交叉熵损失、softmax

#### 熵和信息熵

熵和信息熵本质是一个东西，就是换了个说法而已。

熵：在信息论中则表示事务的不确定性。信息量与信息熵是相对的，告诉你一件事实，你获取了信息量，但减少了熵。或者说，得知一件事实后信息熵减少的量，就是你得到的这个事实所包含的信息的量。

熵的公式：$H(x)=-\sum_{i=1}^{n} P\left(x_{i}\right) \log _{2} P\left(x_{i}\right)$

n:表示随机变量可能的取值
x:表示随机变量
P(x):表示随机变量x的概率函数

**log以10，2或者e为底，对结果熵的判断没有影响**

#### 相对熵和交叉熵
**相对熵就是KL散度**

$D_{K L}(p \| q)=\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(\frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}\right)$

用于衡量两个概率分布之间的差异。

我们把上面的公式展开

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/Snipaste_2023-04-07_10-59-34.png)


$$\begin{aligned} 
D_{K L}(p \| q)&=\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(\frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}\right) \\
               &=\sum_{i=1}^{n}p(x_{i})l o g\,p(x_{i})-\sum_{i=1}^{n}p(x_{i})l o g q(x_{i})\\
               &= -(-\sum_{i=1}^{n}p(x_{i})l o g\,p(x_{i})) - \sum_{i=1}^{n}p(x_{i})l o g q(x_{i})                     \\
               &=  -H(P) + H(P,Q)               \\
               &= H(P,Q) -H(P)              \\
               &= 交叉熵 - 信息熵                      \\
\end{aligned}$$ 

p(x)表示真实概率分布，q(x)表示预测概率分布
**交叉熵刻画的是实际输出（概率）与期望输出（概率）的距离，也就是交叉熵的值越小，两个概率分布就越接近，即拟合的更好。**

当p(x)=q(x)时，相对熵为0
相对熵越小越好，相对熵和交叉熵的差距只有一个常数。那么相对熵达到最小值的适合，也就是交叉熵达到最小的时候。所以对q(x)的优化等效于求交叉熵的最小值，交叉熵的最小值也就是求最大似然估计

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/v2-95c59c2a55a6782d45b20b9e00913da7_1440w.webp)


#### 似然函数、极大似然函数

p(x|θ)也是一个有着两个变量的函数。如果，你将θ设为常量，则你会得到一个概率函数（关于x的函数）；如果，你将x设为常量你将得到似然函数（关于θ的函数）。

概率描述的是：指定参数后，预测即将发生事件的可能性；

似然描述的是：在已知某些观测所得到的结果时，对有关事物的性质的参数进行估计；

极大似然估计是在已知一堆数据和分布类型的前提下，反推最有可能的参数是什么，也就是“它最像这个分布哪组参数下表现出来的数据”。

举个例子：

将抽球结果作为$X$，即离散随机变量，设白球为 
$X=1$，黑球为 $X=0$。假设抽到白球的概率为 $\theta$，$\theta$ 即是未知的需要通过极大似然估计得出的参数。

写出一次预测的似然函数：
$$L(\theta|x)=f(x|\theta)=P(x,\theta)=\theta^{x}*{(1-\theta)}^{(1-x)}$$

这里解释下为什么是这样的：

如果抽到的是白球，就是$X=1$,密度函数是$\theta$，带入公式没有问题；
如果抽到的是黑球，就是$X=0$,密度函数是$1-\theta$，带入公式没有问题；

对于二项分布，出现符合观测情况的，白球出现7次，黑球出现三次的概率密度函数为
$P(X,\theta)=P(x1,\theta)*P(x2,\theta)*..\cdot P(x10,\theta)=\theta^{7}*(1-\theta)^{3}$

写成似然函数形式为：

$L(\theta|X)=P(X,\theta)=\theta^{7}*(1-\theta)^{3}$

#### 似然函数和交叉熵的关系

为了求最大的似然函数，我们往往取对数，最后发现二分类的极大似然函数和二分类交叉熵相同

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/Snipaste_2023-04-07_21-07-24.png)

[原文链接](https://blog.51cto.com/u_15899958/5909794)

## 线性神经网络
### 线性回归

线性回归基于几个简单的假设： 首先，假设自变量$x$
和因变量$y$之间的关系是线性的， 即$y$可以表示为$x$
中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

<!-- 线性模型：$\hat{y}=w_1x_1+...+w_dx_d+b$ -->

**线性回归的关键在于寻找最好的模型参数**，需要两个东西：
（1）一种模型质量的度量方式：**损失函数**
（2）一种能够更新模型以提高模型预测质量的方法：**优化算法**

模型的优化过程就是：随机抽样一个小批量$\beta$，它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数$\eta$，并从当前参数的值中减掉。


## pytorch安装

### cuda安装
[原链接](https://zhuanlan.zhihu.com/p/94220564)

桌面右键打开英伟达控制面板，点击帮助->系统信息->组件

可以看到支持的版本，安装的cuda版本必须小于等于该版本

安装好cuda后，安装cuDNN。
版本要和cuda对应起来

### miniconda和pytorch安装

[原链接](https://zhuanlan.zhihu.com/p/174738684)


[miniconda的镜像](https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/)

要注意安装的是x86_64的版本，一开始装成的x86(32位，一直出问题)

安装pytorch的时候要进入[这里](https://pytorch.org/get-started/previous-versions/)。根据对应的cuda版本来下载，最然根据教程来验证是否安装成果。



### 安装jupyter notebook

安装jupyter notebook有三个办法：

**方法1：**
**为每一个 conda 环境 都安装 jupyter**

上面的安装好以后，使用```conda activate d2l```，激活d2l环境。
用```conda install jupyter```安装一直卡在那，换pip安装。但是还是因为网速原因没成功，可以使用临时换源的办法：

```pip install jupyter -i https://pypi.tuna.tsinghua.edu.cn/simple```



**方法2：**

在base环境安装好jupyter后

```
conda create -n my-conda-env                               # creates new virtual env
conda activate my-conda-env                                # activate environment in terminal
conda install ipykernel                                    # install Python kernel in new conda env
ipython kernel install --user --name=my-conda-env-kernel   # configure Jupyter to use Python kernel
```

然后在base环境运行jupyter，下面两种方式都可以切换环境

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/微信图片_20230315114520.png)

![](https://cdn.jsdelivr.net/gh/gaofeng-lin/picture_bed/img/微信图片_20230315114528.png  )

**缺点是你新建一个环境，就要重复操作一次**

**方法3：**

```
conda activate my-conda-env    # this is the environment for your project and code
conda install ipykernel
conda deactivate

conda activate base      # could be also some other environment
conda install nb_conda_kernels
jupyter notebook
```

注意：这里的 ```conda install nb_conda_kernels``` 是在 base 环境下操作的。

然后就可以进行conda环境求换，方式和法2相同。

本人在使用方法3的时候遇到了问题，web端显示500，命令行显示的关键信息如下：

**ImportError: cannot import name 'contextfilter' from 'jinja2'**

最后的解决方法：
```
conda update nbconvert
```

### 导入torchvision出现错误

cuda和pythorch都安装成功的时候，且gpu也能正常使用。但是运行d2l里面的代码报错：
```
import torch 成功

import torchvision,报错

DLL:找不到模块
```

根据torch版本找到对应的torchvision，然后卸载torchvision再安装，显示没有这个版本。当时安装torch的时候，torchvision也安装了，且版本正确。

**解决办法：**
1. 先查看一下Pillow的版本
```
pip show Pillow
```

如果没有直接安装
```
pip install Pillow
```

如果有，先卸载
```
pip uninstall Pillow
```

再安装
```
pip install Pillow
```

然后检验torchvision是否正常
```
>>> import torchvision
>>> torchvision.__version__
'0.8.2'

```